{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Performance Evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "HtZ2yI1Lgh2I",
        "SqjqAVSQgh2S",
        "Yh4fSgV9gh2a",
        "WlQf5BUtgh2o",
        "plCwrQB_gh23",
        "1_2Y9vQ9gh2_",
        "zY7eJaxsgh3K",
        "hum7bVaAgh3P",
        "GP8EUDXTgh3s",
        "AjIBhaItgh4X",
        "RHrz_JZAgh4j",
        "mx7Dtz5Wgh4r",
        "Onetrj_Agh5A",
        "gESg0k8_gh5H",
        "YoVnDQsigh5x",
        "QQwxdMUkgh6m",
        "1gp-Po0wgh7w"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdPA03q3vRei",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1JuiXnrL99eZzIiRzdtREyLlCYhF6Nc4u?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4yt1psqgh1X",
        "colab_type": "text"
      },
      "source": [
        "# Performance Evaluation, Cross Validation and Hyper - Parameter Tunning\n",
        "In this Notebook, we will learn 3 things:   \n",
        "*  Evaluation metrics\n",
        "*  Cross Validation\n",
        "*  Hyperparameter Tuning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57GJ0KnnpC34",
        "colab_type": "text"
      },
      "source": [
        "### Data Description\n",
        "Here we will use diabetes dataset. Given different medical specifications about a person, we have to predict if the person have diabetes or not.\n",
        "\n",
        "**Different Attributes:**\n",
        "1. Number of times pregnant\n",
        "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
        "3. Diastolic blood pressure (mm Hg)\n",
        "4. Triceps skin fold thickness (mm)\n",
        "5. 2-Hour serum insulin (mu U/ml)\n",
        "6. Body mass index (weight in kg/(height in m)^2)\n",
        "7. Diabetes pedigree function\n",
        "8. Age (years)\n",
        "9. Class variable (0 or 1)\n",
        "\n",
        "All the feature names are numerical. Let's give textual names to these features.\n",
        "*  Number of times pregnant: **num_preg**\n",
        "*  Plasma glucose concentration a 2 hours in an oral glucose tolerance test: **plasma_glucose_conc**\n",
        "*  Diastolic blood pressure (mm Hg): **D_blood_pressure**\n",
        "*  Triceps skin fold thickness (mm): **skin_fold_thickness**\n",
        "*  2-Hour serum insulin (mu U/ml): **serum_insulin**\n",
        "*  Body mass index (weight in kg/(height in m)^2): **body_mass_index**\n",
        "*  Diabetes pedigree function: **pedigree_func**\n",
        "*  Age (years): **age**\n",
        "*  Class variable (0 or 1): **diabetes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYnubTr4oUKY",
        "colab_type": "text"
      },
      "source": [
        "### Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WQljMFOgh1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRSnYQG9okA4",
        "colab_type": "text"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUaCOsuroooy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# since the column names are numerical, we will give our own column names for our understanding\n",
        "col = [\"num_preg\", \"plasma_glucose_conc\", \"D_blood_pressure\", \"skin_fold_thickness\", \"serum_insulin\", \"body_mass_index\", \"pedigree_func\", \"age\", \"diabetes\"]\n",
        "diabetes_data = pd.read_csv(\"https://raw.githubusercontent.com/dphi-official/ML_Models/master/Performance_Evaluation/diabetes.txt\", names = col)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3Gg7tKkv8uw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e2a7745b-7de4-41c2-9e06-dd5a8d8457e3"
      },
      "source": [
        "diabetes_data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_preg</th>\n",
              "      <th>plasma_glucose_conc</th>\n",
              "      <th>D_blood_pressure</th>\n",
              "      <th>skin_fold_thickness</th>\n",
              "      <th>serum_insulin</th>\n",
              "      <th>body_mass_index</th>\n",
              "      <th>pedigree_func</th>\n",
              "      <th>age</th>\n",
              "      <th>diabetes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   num_preg  plasma_glucose_conc  ...  age  diabetes\n",
              "0         6                  148  ...   50         1\n",
              "1         1                   85  ...   31         0\n",
              "2         8                  183  ...   32         1\n",
              "3         1                   89  ...   21         0\n",
              "4         0                  137  ...   33         1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbHhT_yMgh1p",
        "colab_type": "text"
      },
      "source": [
        "**This dataset contains 13 columns and based on different features, it is guessed whether or not a person has Diabetes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67Eap_ejyyJ5",
        "colab_type": "text"
      },
      "source": [
        "### Separating Input variables and output variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly3PQ9DayxWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = diabetes_data.drop('diabetes', axis = 1)\n",
        "y = diabetes_data.diabetes"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOQVck62gh1z",
        "colab_type": "text"
      },
      "source": [
        "#### Split into training and testing (80:20)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AylKRHMgh12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
        "\n",
        "# The below line of code will not need to separate input variables and output variables.\n",
        "# The code is very simple if you remember numpy and pandas session. Indexing dataframe and arrays\n",
        "# x_train, x_test, y_train, y_test = train_test_split(diabetes.iloc[:, :-1], diabetes.iloc[:,-1], test_size=0.2, random_state=3)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3R17-E7kqjD",
        "colab_type": "text"
      },
      "source": [
        "**Note for learners:** Here we have used MLPClassifier from neural_network module of sklearn library. MLP Classifier is also a classification algorithm like logistic regression or decision tree. We will soon learn about Neural Networks and Artificial Neural Networks in the upcoming sessions. So, no need to worry about it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZizeBIHwgh17",
        "colab_type": "text"
      },
      "source": [
        "### Developing a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqeygkPIgh18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "mlp = MLPClassifier(max_iter=1000)\n",
        "mlp.fit(x_train, y_train)\n",
        "y_pred = mlp.predict(x_test)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8-k13W0gh1T",
        "colab_type": "text"
      },
      "source": [
        "## Performance Evaluation\n",
        "Evaluating performance of the machine learning model that we have built is an essential part of any machine learning project. Performance of our model is done using some evaluation metrics. Accuracy score is one among them.\n",
        "\n",
        "**Why accuracy score is not a good evaluation metric?**\n",
        "\n",
        "Our model may give satisfying results if we use accuracy score for a particular dataset but at the same time accuracy score is not a good measure of evaluation for some particular dataset like fraud detection that we discussed during class imbalance problem. Let's consider the same dataset, suppose we have 1000 transaction in the dataset. Out of 1000 transactions 20 transactions are fraud transactions. Now let's say you build a model which predicted all the 1000 transactions as not fraud transaction, for 980 transaction which were not fraud, the prediction is correct while the transaction which were actually fraud are also predicted as not fraud. The accuracy is nothing but total correct prediction divided by total prediction. In this case we have total prediction as 1000 (as we have 1000 transactions) while total correct prediction is 980, resulting the accuracy score of 980/1000 = 0.98. The model is giving 98% of accuracy. Do you think the model is good? No, because our model could not notice the transaction which were actually fraud. \n",
        "\n",
        "Here we will discuss some other metrices for both classification and regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D9DMXXigh1Z",
        "colab_type": "text"
      },
      "source": [
        "## 1. Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD9O15axgh2G",
        "colab_type": "text"
      },
      "source": [
        "**All performance metrics in sklearn are to be written in the same way -**  \n",
        "> ``` metric_function(true_label, predicted_labels) ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V1Az3YsBUgP",
        "colab_type": "text"
      },
      "source": [
        "Below are the metrics for classification problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtZ2yI1Lgh2I",
        "colab_type": "text"
      },
      "source": [
        "### Confusion Matrix\n",
        "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing.\n",
        "\n",
        "Further reading about confusion matrix and its related terminologies: \n",
        "1. https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/\n",
        "2. https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKr_nFuRgh2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12hGE_5rgh2O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0ae14c42-0bc4-4ecc-812b-466e4ecdee72"
      },
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()     # ravel() is used to convert a 2D array to 1D array. The output by confusion matrix is a 2D array.\n",
        "print(\"True Positive\", tp)\n",
        "print(\"True Negative\", tn)\n",
        "print(\"False Positive\", fp)\n",
        "print(\"False Negative\", fn)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True Positive 21\n",
            "True Negative 85\n",
            "False Positive 7\n",
            "False Negative 41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqjqAVSQgh2S",
        "colab_type": "text"
      },
      "source": [
        "### Accuracy\n",
        "\\begin{align}\n",
        "Accuracy = \\frac{TP+TN}{TP+TN+FN+FP}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AX3DELNUgh2U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e453fc6-9ee2-4e1e-9531-5967515e13aa"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "acc"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6883116883116883"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNFBGQaCEjCA",
        "colab_type": "text"
      },
      "source": [
        "**When is it good to use accuracy score as a model evaluation metric?**\n",
        "1. The classifications in the dataset is nearly symmetrical (means equal distribution of all the classes).\n",
        "2. The false positive and false negative on test data are nearly equal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh4fSgV9gh2a",
        "colab_type": "text"
      },
      "source": [
        "### Recall (Sensitivity)\n",
        "\\begin{align}\n",
        "Sensitivity = \\frac{TP}{TP+FN}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGBdDw9hgh2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import recall_score"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk9ffog9gh2g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f7fff70-efdc-4434-a8fc-84b4b0426f32"
      },
      "source": [
        "recall_score(y_test, y_pred)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3387096774193548"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlQf5BUtgh2o",
        "colab_type": "text"
      },
      "source": [
        "### Specificity\n",
        "sklearn does not have an inbuild function for Specificity. But by adding parameter pos_label =0 to the recall function, we treat that as the positive class, and hence gives the correct output\n",
        "\\begin{align}\n",
        "Specificity = \\frac{TN}{TN+FP}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHTxK6NHgh2q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c257a68f-a910-48b9-aae5-f60cf7eee5a9"
      },
      "source": [
        "print(\"Specificity with recall pos label=0: \",recall_score(y_test, y_pred, pos_label=0))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Specificity with recall pos label=0:  0.9239130434782609\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UAM0Kz2gh2u",
        "colab_type": "text"
      },
      "source": [
        "**Checking with formulas (tn , fp from confusion matrix):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrthV4dhgh2v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e14dadc7-ce97-4057-9014-8d60cb13752c"
      },
      "source": [
        "print(\"Specificity with Formulas: \", tn/(tn+fp))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Specificity with Formulas:  0.9239130434782609\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7QrITQBgh21",
        "colab_type": "text"
      },
      "source": [
        "They are the same! You can use either one of them!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plCwrQB_gh23",
        "colab_type": "text"
      },
      "source": [
        "### Precision\n",
        "\\begin{align}\n",
        "Precision = \\frac{TP}{TP+FP}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14pynqe6gh25",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f0a0df1d-a9a2-4c64-adfe-8e5c271cdc07"
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "precision_score(y_test, y_pred)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.75"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_2Y9vQ9gh2_",
        "colab_type": "text"
      },
      "source": [
        "### Imbalanced Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwMwh4mhgh3A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "adcb9054-0209-410d-851f-c738ef8ecafb"
      },
      "source": [
        "diabetes_data.iloc[:,-1].value_counts()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    500\n",
              "1    268\n",
              "Name: diabetes, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY7eJaxsgh3K",
        "colab_type": "text"
      },
      "source": [
        "### Matthews Correlation Coefficient\n",
        "\\begin{align}\n",
        "MCC = \\frac{(TP*TN)-(FP*FN)}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vr62zEHWgh3L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c65053a-33a7-40f0-f7a2-a9044443ea3d"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "print(\"MCC Score: \",matthews_corrcoef(y_test, y_pred))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC Score:  0.3339317909634408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hum7bVaAgh3P",
        "colab_type": "text"
      },
      "source": [
        "### F1 Score\n",
        "It is the harmonic mean of Precision and recall\n",
        "\n",
        "\\begin{align}\n",
        "Precision = \\frac{2*Precision*Recall}{Precision+Recall}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3A9uIKwgh3Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f755617-b37f-4bf3-e89e-acb7139b850d"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "print(\"F1 Score: \",f1_score(y_test, y_pred))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 Score:  0.4666666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVcVsBEhgh3V",
        "colab_type": "text"
      },
      "source": [
        "## Area Under the Curve (Reciever Operating Characterstics)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YypfoTaVgh3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import plot_roc_curve"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZPwqn0Dgh3c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "2d0c5375-77f5-4353-905e-3027bfcce8a4"
      },
      "source": [
        "plot_roc_curve(mlp, x_test, y_test)\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHrohQufgh3l",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Regression Evaluation Metrics "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUc_2Fz7gh3m",
        "colab_type": "text"
      },
      "source": [
        "Wine Dataset  \n",
        "    <b> Predictor Variable: </b> Quality (Tells quality of wine)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMZbaFOcgh3o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "57a45b94-0f83-4c9c-d42d-940cff64867b"
      },
      "source": [
        "wine = pd.read_csv(\"https://raw.githubusercontent.com/dphi-official/ML_Models/master/Performance_Evaluation/winequality.csv\", sep=\";\")\n",
        "wine.head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.0</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.36</td>\n",
              "      <td>20.7</td>\n",
              "      <td>0.045</td>\n",
              "      <td>45.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>1.0010</td>\n",
              "      <td>3.00</td>\n",
              "      <td>0.45</td>\n",
              "      <td>8.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.3</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.34</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.049</td>\n",
              "      <td>14.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.9940</td>\n",
              "      <td>3.30</td>\n",
              "      <td>0.49</td>\n",
              "      <td>9.5</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.1</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.40</td>\n",
              "      <td>6.9</td>\n",
              "      <td>0.050</td>\n",
              "      <td>30.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0.9951</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.44</td>\n",
              "      <td>10.1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.5</td>\n",
              "      <td>0.058</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.5</td>\n",
              "      <td>0.058</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fixed acidity  volatile acidity  citric acid  ...  sulphates  alcohol  quality\n",
              "0            7.0              0.27         0.36  ...       0.45      8.8        6\n",
              "1            6.3              0.30         0.34  ...       0.49      9.5        6\n",
              "2            8.1              0.28         0.40  ...       0.44     10.1        6\n",
              "3            7.2              0.23         0.32  ...       0.40      9.9        6\n",
              "4            7.2              0.23         0.32  ...       0.40      9.9        6\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP8EUDXTgh3s",
        "colab_type": "text"
      },
      "source": [
        "#### Split into training and testing (80:20)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGgMF3Rzgh3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here we are performing both separation of input and output variable and the splitting.\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(wine.iloc[:, :-1], wine.iloc[:,-1], test_size=0.2, random_state=3)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnuT2aaQgh3z",
        "colab_type": "text"
      },
      "source": [
        "Creating a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsRa3Pe0gh32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PMAIOYRgh4B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3f3ef5c0-0fe0-4aa4-9b9b-afa1d2eb8f83"
      },
      "source": [
        "lr = LinearRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "y_pred = lr.predict(x_test)\n",
        "y_pred[:10]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.44455619, 5.57868309, 5.99091469, 5.19864346, 6.0666099 ,\n",
              "       5.01639077, 5.68416174, 6.26611011, 5.97010538, 5.65519351])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEQuSCzMgh4R",
        "colab_type": "text"
      },
      "source": [
        "## Performance Measurement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FZ86Xpbgh4U",
        "colab_type": "text"
      },
      "source": [
        "Let y = Actual Value,  $\\tilde{y}$ = Predicted Value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjIBhaItgh4X",
        "colab_type": "text"
      },
      "source": [
        "### Mean Absolute Error  \n",
        "*  MAE is the absolute difference between the target value and the value predicted by the model.  \n",
        "*  The MAE is more robust to outliers and does not penalize the errors as extremely as mse\n",
        "\\begin{align}\n",
        "MAE  = \\frac{1}{n}\\sum|y-\\tilde{y}| \n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNOb1yb2gh4Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82acb1ee-cecd-4a11-9448-d4ba33fa9920"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "mean_absolute_error(y_test, y_pred)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5972358558776472"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHrz_JZAgh4j",
        "colab_type": "text"
      },
      "source": [
        "### Mean Squared Error\n",
        "*  It is simply the average of the squared difference between the target value and the value predicted by the regression model. \n",
        "*  As it squares the differences, it penalizes even a small error which leads to over-estimation of how bad the model is.\n",
        "*  MSE or Mean Squared Error is one of the most preferred metrics for regression tasks. \n",
        "\\begin{align}\n",
        "MSE & = \\frac{1}{n}\\sum(y-\\tilde{y})^2\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHZcMiDrgh4k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "17e1926d-fe7e-470e-82e6-8394391af81f"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"Mean Squared Error: \",mean_squared_error(y_test, y_pred))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Squared Error:  0.5906658099548077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx7Dtz5Wgh4r",
        "colab_type": "text"
      },
      "source": [
        "### Root Mean Square Error\n",
        "*  RMSE is the square root of the averaged squared difference between the target value and the value predicted by the model. \n",
        "*  It is preferred more in some cases because the errors are first squared before averaging which poses a high penalty on large errors.\n",
        "*  This implies that RMSE is useful when large errors are undesired.\n",
        "\\begin{align}\n",
        "RMSE  = \\sqrt{\\frac{1}{n}\\sum(y-\\tilde{y})^2}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "EfQI1JFzgh4u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f6299173-2152-489d-ca97-4ef038209300"
      },
      "source": [
        "print(\"Root Mean Squared Error: \",mean_squared_error(y_test, y_pred, squared=False))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Root Mean Squared Error:  0.7685478579469256\n",
            "Root Mean Squared Error:  0.7685478579469256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Onetrj_Agh5A",
        "colab_type": "text"
      },
      "source": [
        "### R Squared\n",
        "<li>The metric helps us to compare our current model with a constant baseline and tells us how much our model is better\n",
        "\\begin{align}\n",
        "R^2 = 1 - \\frac{MSE(Model)}{MSE(Baseline)}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmTpqG2rgh5B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a02a6e2-e1b1-474e-8f79-92bb71c15dc1"
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_test, y_pred)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2832037191111023"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VoVhiVKgh5H",
        "colab_type": "text"
      },
      "source": [
        "# 2. Cross Validation\n",
        "Usually, our data is divided into Train and Test Sets.\n",
        "The Train set is further divided into Train and Validation set. \n",
        "\n",
        "The Validation Set helps us in selecting good parameters/tune the parameters for our model.\n",
        "\n",
        "This Three fold set can be seen in the figure below:\n",
        "\n",
        "![train-test-val](https://amueller.github.io/ml-training-intro/slides/images/threefold_split.png)\n",
        "\n",
        "Our dataset should be as large as possible to train the model and removing considerable part of it for validation poses a problem of losing valuable portion of data that we would prefer to be able to train. \n",
        "\n",
        "In order to address this issue, we use the Cross validation technique. Cross Validation has a number of types out of which we'll be using K-fold cross validation today.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gESg0k8_gh5H",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 K-Fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKX8Gjyugh5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_validate"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7kl2Ah_gh5Q",
        "colab_type": "text"
      },
      "source": [
        "<blockquote> We can also import cross_val_score from the same library, but it only allows a single scorer to be implemented. So we are using cross_validate </blockquote>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GiU8TBWgh5Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "88628060-694c-4f07-a1a5-057d8fbc08f0"
      },
      "source": [
        "cv_results = cross_validate(mlp, X, y, cv=10, scoring=[\"accuracy\", \"precision\", \"recall\"])\n",
        "cv_results"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fit_time': array([0.29621458, 0.58519506, 0.2758708 , 0.71810746, 0.45174599,\n",
              "        0.6917429 , 0.34141088, 0.3458221 , 0.43011928, 0.27908921]),\n",
              " 'score_time': array([0.00480342, 0.0043273 , 0.00410509, 0.00405145, 0.00408554,\n",
              "        0.00405836, 0.00369906, 0.00416088, 0.00384641, 0.00388813]),\n",
              " 'test_accuracy': array([0.64935065, 0.7012987 , 0.68831169, 0.72727273, 0.7012987 ,\n",
              "        0.7012987 , 0.75324675, 0.71428571, 0.75      , 0.69736842]),\n",
              " 'test_precision': array([0.5       , 0.66666667, 0.57142857, 0.6875    , 0.57142857,\n",
              "        1.        , 0.83333333, 0.56756757, 0.62068966, 0.61538462]),\n",
              " 'test_recall': array([0.7037037 , 0.2962963 , 0.44444444, 0.40740741, 0.59259259,\n",
              "        0.14814815, 0.37037037, 0.77777778, 0.69230769, 0.30769231])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2N7yzQRgh5V",
        "colab_type": "text"
      },
      "source": [
        "**cv=10 is provided, which means we are performing 10 fold cross validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxN-rYmigh5W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "544e8651-f146-41cd-cd79-1dbf3cab5961"
      },
      "source": [
        "print(\"Accuracy: \", cv_results[\"test_accuracy\"].mean())\n",
        "print(\"Precision: \", cv_results[\"test_precision\"].mean())\n",
        "print(\"Recall: \", cv_results[\"test_recall\"].mean())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7083732057416269\n",
            "Precision:  0.663399898098174\n",
            "Recall:  0.47407407407407404\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrNp0R93gh5a",
        "colab_type": "text"
      },
      "source": [
        "**For all valid scoring options - use the following:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mz4G3U5gh5e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "19733abb-f81e-4672-ba1b-f2e36b12f5da"
      },
      "source": [
        "import sklearn.metrics as m\n",
        "m.SCORERS.keys()\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['explained_variance', 'r2', 'max_error', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error', 'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'neg_brier_score', 'adjusted_rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygp5tnDdgh5q",
        "colab_type": "text"
      },
      "source": [
        "For more complicated scoring metrics (such as specificity, which isn't explicilty provided by sklearn), or to create your own metrics, \n",
        "http://scikit-learn.org/stable/modules/model_evaluation.html#using-multiple-metric-evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoVnDQsigh5x",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Leave One Out Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDxbxH6ggh5x",
        "colab_type": "text"
      },
      "source": [
        "<blockquote>This code takes a long time to run, you can either skip running this part and directly just see the printed results, or wait for 10-15 mins for this to run </blockquote>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZEbpVaygh5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import LeaveOneOut"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "vBvtai8Xgh55",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e6144369-f310-41f0-d090-82fdd547f27f"
      },
      "source": [
        "cv_results = cross_validate(mlp, X, y,\n",
        "                            cv=LeaveOneOut(), scoring=[\"accuracy\"])\n",
        "cv_results"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fit_time': array([0.50730586, 0.39103937, 0.3785851 , 0.34293866, 0.67757607,\n",
              "        0.45510221, 0.31566334, 0.3564589 , 0.59180951, 0.64962935,\n",
              "        0.79952979, 0.38669538, 0.37384248, 0.46679831, 0.31697583,\n",
              "        0.69750547, 0.38226819, 0.4034586 , 0.72357416, 0.51584172,\n",
              "        0.34693027, 0.40484476, 0.76911044, 0.44652915, 0.4016099 ,\n",
              "        0.45877767, 0.4892385 , 0.34868908, 0.48699975, 0.56012654,\n",
              "        0.28004527, 0.38736725, 0.46224761, 0.37005901, 0.56642556,\n",
              "        0.86166453, 0.42618465, 0.60612273, 0.56659317, 0.28077841,\n",
              "        0.51177859, 0.71510482, 0.49838495, 0.34566236, 0.96387982,\n",
              "        0.40084004, 0.39566731, 0.62272024, 0.58541226, 0.40983129,\n",
              "        0.63371897, 0.50266194, 0.50982666, 0.23233771, 0.45473289,\n",
              "        0.77129245, 0.33790708, 0.5541513 , 0.30765033, 0.58533669,\n",
              "        0.62194061, 0.36196113, 0.6268785 , 0.50821567, 0.50683355,\n",
              "        0.88323975, 0.40367079, 0.50237656, 0.42735982, 0.6504724 ,\n",
              "        0.38024497, 0.57250094, 0.64503789, 0.82746315, 0.51225972,\n",
              "        0.62388301, 0.44489169, 0.87477732, 0.52063894, 0.49461985,\n",
              "        0.44234705, 0.41675043, 0.5911231 , 0.45367765, 0.36274457,\n",
              "        0.31572509, 0.68789387, 0.40916586, 0.54134703, 0.27804065,\n",
              "        0.69607925, 0.62398815, 0.54078293, 0.31348586, 0.39026928,\n",
              "        0.34479952, 0.69779849, 0.27567315, 0.45785594, 0.75111723,\n",
              "        0.29828572, 0.63067961, 0.27890182, 0.77096033, 0.34319282,\n",
              "        0.41855359, 0.17844105, 0.55085397, 0.6166451 , 0.31970096,\n",
              "        0.87324333, 0.5991354 , 0.41491246, 0.27379632, 0.24299026,\n",
              "        0.74814296, 0.55430055, 0.60948658, 0.67595696, 0.86298013,\n",
              "        0.64194298, 0.58206558, 0.43669724, 0.33052778, 0.57572579,\n",
              "        0.3123908 , 0.32292223, 0.48008204, 0.50641227, 0.53309989,\n",
              "        0.53254104, 0.43763161, 0.56141305, 0.37600875, 0.43088937,\n",
              "        0.44715738, 0.2873199 , 0.40292239, 0.38188171, 0.80430937,\n",
              "        0.26758742, 0.49948406, 0.78018093, 0.6871376 , 0.48841763,\n",
              "        0.32165408, 0.97195125, 0.42114782, 0.39670229, 0.29362059,\n",
              "        0.3063333 , 0.57730341, 0.56444192, 0.25047088, 0.55816102,\n",
              "        0.49621725, 0.20990252, 0.49072027, 0.81003022, 0.56464791,\n",
              "        0.48202491, 0.43141675, 0.59102821, 0.30618429, 0.64509392,\n",
              "        0.82662058, 0.49485874, 0.46399879, 0.75833702, 0.76691151,\n",
              "        0.63236356, 0.61851239, 0.76055837, 0.33469605, 0.54722166,\n",
              "        0.35061097, 0.56533551, 0.39492869, 0.47618079, 0.68438935,\n",
              "        0.43808556, 0.25239158, 0.73235416, 0.31346464, 0.38547397,\n",
              "        0.75652933, 0.45650458, 0.59283566, 0.45001841, 0.49798036,\n",
              "        0.42023349, 0.50905728, 0.50886297, 0.30719328, 0.56281352,\n",
              "        0.71389055, 0.46402216, 0.40586495, 0.60686803, 0.74238014,\n",
              "        0.64801478, 0.79747224, 0.42937517, 0.41171408, 0.45035911,\n",
              "        0.6549809 , 0.51883364, 0.50456285, 0.27675509, 0.62942362,\n",
              "        0.50059199, 0.14946032, 0.76414132, 0.58609486, 0.60700011,\n",
              "        0.50814891, 0.55753517, 0.37092447, 0.43860364, 0.24928856,\n",
              "        0.36576271, 0.6262331 , 0.83892703, 0.50375032, 0.23077893,\n",
              "        0.36231923, 0.48704195, 0.25894523, 0.59857607, 0.48744583,\n",
              "        0.50843573, 0.23250961, 0.30746293, 0.52059793, 0.54861355,\n",
              "        0.49909568, 0.33797383, 0.19670296, 0.77712154, 0.25072885,\n",
              "        0.43755221, 0.37105823, 0.51000452, 0.61033463, 0.48814917,\n",
              "        0.41725993, 0.40461445, 0.41689396, 0.33611917, 0.38669658,\n",
              "        0.44154882, 0.27703404, 0.52403283, 0.66037607, 0.62762189,\n",
              "        0.3710773 , 0.61725163, 0.30788946, 0.368922  , 0.67218232,\n",
              "        0.49745011, 0.51531076, 0.32692504, 0.38342071, 0.29506731,\n",
              "        0.62057686, 0.48418021, 0.39111996, 0.36864638, 0.68620992,\n",
              "        0.32239056, 0.63960195, 0.95251751, 0.50222778, 0.61470985,\n",
              "        0.58875871, 0.71925592, 0.37200975, 0.54626894, 0.60649037,\n",
              "        0.31521583, 0.79248261, 0.3425045 , 0.46332383, 0.50363755,\n",
              "        0.35989189, 0.74384904, 0.43089223, 0.42081523, 0.45888448,\n",
              "        0.44502521, 0.33353019, 0.55650544, 0.44638038, 0.78658557,\n",
              "        0.49755287, 0.35301375, 0.69880962, 0.2718637 , 0.26369977,\n",
              "        0.4858892 , 0.61532903, 0.50645494, 0.59941983, 0.82268739,\n",
              "        0.52372217, 0.31543541, 0.42105961, 0.36198783, 0.37601066,\n",
              "        0.25075912, 0.45410252, 0.70465612, 0.43865848, 0.59968066,\n",
              "        0.44467163, 0.35018492, 0.51401281, 0.32321644, 0.30682516,\n",
              "        0.51369858, 0.25939393, 0.28081608, 0.38474655, 0.70008588,\n",
              "        0.33331513, 0.57876468, 0.21211481, 0.72450757, 0.95605063,\n",
              "        0.47700286, 0.26263976, 0.2472651 , 0.49544764, 0.24907517,\n",
              "        0.30414867, 0.34328294, 0.43865752, 0.44693375, 0.97893524,\n",
              "        0.56203175, 0.35311055, 0.58508515, 0.28500342, 0.36895633,\n",
              "        0.32483625, 0.31781936, 0.32554173, 0.53481317, 0.51134324,\n",
              "        0.61085176, 1.02217984, 0.43286133, 0.22543788, 0.5257926 ,\n",
              "        0.66589355, 0.31467938, 0.43292284, 0.45900297, 0.50305867,\n",
              "        0.48161197, 0.5481391 , 0.42079878, 0.57495832, 0.57227898,\n",
              "        0.54578614, 0.30687761, 0.65107441, 0.30479407, 0.56959867,\n",
              "        0.36357474, 0.25388432, 0.51051378, 0.28154397, 0.37497473,\n",
              "        0.31706595, 0.55828166, 0.48334193, 0.71841645, 0.45801878,\n",
              "        0.26609445, 0.59569454, 0.44561839, 0.45351195, 0.6018889 ,\n",
              "        0.44711733, 0.41875815, 0.70146799, 0.42199063, 0.67317152,\n",
              "        0.18649769, 0.66864896, 0.34339571, 0.35656071, 0.43449926,\n",
              "        0.38133502, 0.37107015, 0.49314165, 0.58893275, 0.2436471 ,\n",
              "        0.33332443, 0.5997262 , 0.69046211, 0.62640738, 0.42875743,\n",
              "        0.46874118, 1.08774304, 0.33255839, 0.40647817, 0.35167408,\n",
              "        0.61131406, 0.55270839, 0.87914586, 0.25186753, 0.56511378,\n",
              "        0.5122416 , 0.57520986, 0.30751348, 0.92692351, 0.38100219,\n",
              "        0.33869648, 0.72362614, 0.65718865, 0.25596404, 0.35867643,\n",
              "        0.5180738 , 0.4628458 , 0.50547576, 0.24320269, 0.27857685,\n",
              "        0.62122488, 0.48932171, 0.40939784, 0.54383969, 0.53395247,\n",
              "        0.82250166, 0.72553897, 0.42030025, 0.55875111, 0.38659358,\n",
              "        0.61839128, 0.46889019, 0.50562644, 0.52584624, 0.4758029 ,\n",
              "        0.42589307, 0.3759923 , 0.51504707, 0.51852775, 0.68513417,\n",
              "        0.3309052 , 0.52296805, 0.40278077, 0.52860427, 0.44890475,\n",
              "        0.73085928, 0.359375  , 0.52518964, 0.75684977, 0.54009771,\n",
              "        0.53527355, 0.51880932, 0.60084105, 0.54684639, 0.60830641,\n",
              "        0.31442094, 0.26865578, 0.22748637, 0.47475886, 0.56038213,\n",
              "        0.65668488, 1.0460217 , 0.26534772, 0.56569171, 0.60324502,\n",
              "        0.84195137, 0.42729378, 0.37988257, 0.54952765, 0.2602489 ,\n",
              "        0.82531643, 0.58734155, 0.41226411, 0.60308766, 0.28513288,\n",
              "        0.6555593 , 0.57859397, 0.72948909, 0.3710885 , 0.46684003,\n",
              "        0.65351462, 0.46801805, 0.33993411, 0.49638057, 0.43207169,\n",
              "        0.41521955, 0.62563372, 0.51783466, 1.11161733, 0.61885118,\n",
              "        0.62488151, 0.39372802, 0.44801497, 0.30019617, 0.45743299,\n",
              "        0.32074165, 0.6057539 , 0.97192812, 0.46601915, 0.44294024,\n",
              "        0.62417483, 0.20913672, 0.47826767, 0.41153669, 0.87734222,\n",
              "        0.53514171, 0.25703073, 0.59059811, 0.51224327, 0.53991437,\n",
              "        0.5196619 , 0.3775382 , 0.5793736 , 0.43826866, 0.46364427,\n",
              "        0.537117  , 0.39658785, 0.94859099, 0.27196002, 0.25691009,\n",
              "        0.65624547, 0.92491817, 0.42414045, 0.71806431, 0.44650698,\n",
              "        0.4644599 , 0.39326262, 0.29394555, 0.88810468, 0.70407152,\n",
              "        0.31758308, 0.55566859, 0.44598293, 0.44850516, 0.33450389,\n",
              "        0.40363312, 0.2563808 , 0.3209362 , 0.36650252, 0.56945252,\n",
              "        0.40718579, 0.27715802, 0.28635859, 0.34388041, 0.45333982,\n",
              "        0.68478298, 0.44086146, 0.3939147 , 0.45775056, 0.93447423,\n",
              "        0.90768385, 0.83178592, 0.39524221, 0.6199646 , 0.35169244,\n",
              "        0.33078361, 0.35810184, 0.84987569, 0.35482359, 0.68037558,\n",
              "        0.47473454, 0.78890491, 0.45332432, 0.59973884, 0.5827713 ,\n",
              "        0.48593903, 0.38549614, 0.55757856, 0.3684876 , 0.70169735,\n",
              "        0.39629841, 0.54793859, 0.40167665, 0.4215889 , 0.52615881,\n",
              "        0.38079691, 0.52632761, 0.44794536, 0.53375292, 0.34563065,\n",
              "        0.34101582, 0.42072964, 0.64180064, 0.24822593, 0.68584967,\n",
              "        0.64121628, 0.29628849, 0.6263957 , 0.51207614, 0.6671443 ,\n",
              "        0.29216599, 0.38482285, 0.36412525, 0.56165886, 0.56921315,\n",
              "        0.27161312, 0.43744254, 0.67779136, 0.5459795 , 0.39110637,\n",
              "        0.44673824, 0.54915643, 0.62312555, 0.43569183, 0.53763771,\n",
              "        0.37282491, 0.56465149, 0.49840307, 0.6091156 , 0.55153823,\n",
              "        0.78704262, 0.61454463, 0.92076707, 0.87946177, 1.10402942,\n",
              "        0.65405703, 0.95089793, 1.38472152, 1.13192487, 1.40933418,\n",
              "        0.92431974, 1.60535908, 1.15412736, 0.55259299, 0.32905436,\n",
              "        0.35290551, 0.55761623, 0.68772769, 0.52233052, 0.82913446,\n",
              "        0.86952925, 0.27097392, 0.20929837, 0.32807541, 0.84722948,\n",
              "        0.43062472, 0.67721915, 0.65864468, 0.65829182, 0.64823341,\n",
              "        0.68727016, 0.30998611, 0.56258249, 0.34317708, 0.36394167,\n",
              "        0.43214321, 0.33515334, 0.89036584, 0.49955678, 0.4777596 ,\n",
              "        0.61316705, 0.90435982, 0.43014526, 0.67675805, 0.45163298,\n",
              "        0.3952775 , 0.38203526, 0.51322222, 0.74476695, 0.44127774,\n",
              "        0.79073286, 0.69823337, 0.28499484, 0.56968617, 0.43776846,\n",
              "        0.30326676, 0.50333905, 0.3099544 , 0.3387289 , 0.61779308,\n",
              "        0.51024199, 0.23256254, 0.51197863, 0.32010436, 0.45241046,\n",
              "        0.46243334, 0.53936982, 0.75128508, 0.25017214, 0.53436303,\n",
              "        0.27920246, 0.34990597, 0.5100584 , 0.33156848, 0.34704375,\n",
              "        0.38700843, 0.22534871, 0.4167738 , 0.39090204, 0.66232443,\n",
              "        0.75681567, 0.36293888, 0.35363173, 0.36528397, 0.49612546,\n",
              "        0.33282375, 0.34284687, 0.80196834, 0.20854759, 0.27977562,\n",
              "        0.55182481, 0.6222415 , 0.74293113, 0.69568133, 0.40939522,\n",
              "        0.41072559, 0.72633553, 0.34529471, 0.45821595, 0.78345156,\n",
              "        0.50869346, 0.73004174, 0.28606701, 0.51381993, 0.45058227,\n",
              "        0.5762291 , 0.65905762, 0.5139811 , 0.39514589, 0.5609746 ,\n",
              "        0.61803222, 0.75576353, 0.47945166, 0.32181883, 0.37864971,\n",
              "        0.60092211, 0.55325818, 0.58507299, 1.04618669, 0.4217937 ,\n",
              "        0.43603182, 0.47723913, 0.54532003, 0.65080047, 0.44284749,\n",
              "        0.48901558, 0.67229056, 0.65569568, 0.44925737, 0.48109078,\n",
              "        0.35198069, 0.30822539, 0.40788245, 0.17979026, 0.658952  ,\n",
              "        0.26750445, 0.78686118, 0.4607234 , 0.62958574, 0.19917965,\n",
              "        0.45422101, 0.444911  , 0.7776618 , 0.62228775, 0.65484333,\n",
              "        0.65521836, 1.01713729, 0.64643264]),\n",
              " 'score_time': array([0.00173497, 0.00175524, 0.00165486, 0.00155067, 0.00159764,\n",
              "        0.00186968, 0.0018754 , 0.00180984, 0.00173616, 0.00187302,\n",
              "        0.00188565, 0.00183892, 0.00177193, 0.00172758, 0.00181389,\n",
              "        0.00171685, 0.00181818, 0.00162911, 0.00175858, 0.00172687,\n",
              "        0.00170469, 0.0016923 , 0.001683  , 0.0017004 , 0.00169683,\n",
              "        0.00192165, 0.00177789, 0.00162315, 0.00171518, 0.00177479,\n",
              "        0.00188994, 0.00179315, 0.00164962, 0.00178123, 0.00175357,\n",
              "        0.00169659, 0.00148749, 0.00163126, 0.00166702, 0.00181246,\n",
              "        0.0019691 , 0.00176048, 0.00167179, 0.00178695, 0.00164151,\n",
              "        0.0018065 , 0.00181651, 0.00172472, 0.0017066 , 0.00163627,\n",
              "        0.00183034, 0.0018065 , 0.00177813, 0.00173426, 0.00174165,\n",
              "        0.0016551 , 0.00169802, 0.00188208, 0.00173545, 0.00179577,\n",
              "        0.00186253, 0.0016458 , 0.00160813, 0.00191331, 0.00179148,\n",
              "        0.00254512, 0.00174165, 0.00181985, 0.00173497, 0.00169611,\n",
              "        0.00171518, 0.0015192 , 0.0016737 , 0.0019052 , 0.00162959,\n",
              "        0.00177836, 0.0018611 , 0.0018394 , 0.00176144, 0.00181651,\n",
              "        0.00178456, 0.00168943, 0.00184059, 0.00186253, 0.00193572,\n",
              "        0.00171924, 0.00171542, 0.00194407, 0.00170875, 0.00171399,\n",
              "        0.00182247, 0.0016396 , 0.00172949, 0.00179744, 0.0017767 ,\n",
              "        0.001719  , 0.0034256 , 0.00182748, 0.00178337, 0.00180578,\n",
              "        0.00183702, 0.00176239, 0.0017767 , 0.00186062, 0.00362682,\n",
              "        0.00176024, 0.00178719, 0.00175643, 0.00186157, 0.00177908,\n",
              "        0.00169444, 0.00173974, 0.00174689, 0.00176072, 0.00170732,\n",
              "        0.00175738, 0.00182652, 0.00186276, 0.00171041, 0.00182438,\n",
              "        0.00172424, 0.0016613 , 0.00174427, 0.00181508, 0.00189614,\n",
              "        0.00185323, 0.00171161, 0.00180435, 0.0019846 , 0.00179815,\n",
              "        0.00172281, 0.00173068, 0.00183725, 0.00175071, 0.00186419,\n",
              "        0.00160909, 0.00177264, 0.00171328, 0.00178003, 0.00178409,\n",
              "        0.00171304, 0.00184107, 0.00173926, 0.00179029, 0.00177503,\n",
              "        0.00186419, 0.00180697, 0.00177073, 0.00172043, 0.00187969,\n",
              "        0.001863  , 0.00181127, 0.00176716, 0.00181365, 0.00181484,\n",
              "        0.00183105, 0.00180769, 0.00174332, 0.00184727, 0.00178504,\n",
              "        0.00175309, 0.00176883, 0.00192642, 0.0017519 , 0.00176191,\n",
              "        0.00180387, 0.00180292, 0.00180888, 0.00182438, 0.00178003,\n",
              "        0.00179648, 0.00170207, 0.00181675, 0.00176549, 0.00185561,\n",
              "        0.00181818, 0.00184488, 0.00190902, 0.00178051, 0.00173044,\n",
              "        0.0018785 , 0.00163937, 0.00172257, 0.00170112, 0.00189161,\n",
              "        0.00175309, 0.00177646, 0.00179577, 0.00182605, 0.00163174,\n",
              "        0.00199103, 0.00183749, 0.00187516, 0.00171924, 0.00179291,\n",
              "        0.00183415, 0.00182939, 0.00176501, 0.00176072, 0.00178337,\n",
              "        0.00186396, 0.00168371, 0.00167942, 0.00167942, 0.00177121,\n",
              "        0.00181174, 0.00171065, 0.00191712, 0.00195765, 0.00176167,\n",
              "        0.00177026, 0.00166965, 0.00179935, 0.0017612 , 0.0018115 ,\n",
              "        0.00179338, 0.00177479, 0.00179935, 0.00190449, 0.00196958,\n",
              "        0.0017252 , 0.00186396, 0.00174451, 0.00181198, 0.00173354,\n",
              "        0.00175214, 0.00189972, 0.00171232, 0.00171232, 0.00168347,\n",
              "        0.00172544, 0.00173616, 0.00201201, 0.00166845, 0.0017519 ,\n",
              "        0.00176954, 0.00183463, 0.0016005 , 0.00175405, 0.00169778,\n",
              "        0.00167179, 0.00176382, 0.00188446, 0.0016582 , 0.00199366,\n",
              "        0.00192428, 0.00171232, 0.00178242, 0.00175762, 0.00174379,\n",
              "        0.00172472, 0.00175428, 0.00184464, 0.00170279, 0.00180578,\n",
              "        0.00175786, 0.00177121, 0.00177789, 0.00168133, 0.00169659,\n",
              "        0.00176573, 0.00180507, 0.00170755, 0.00200748, 0.00174785,\n",
              "        0.00180197, 0.00176263, 0.00169873, 0.0017302 , 0.00196552,\n",
              "        0.00182962, 0.00170374, 0.00189137, 0.00170493, 0.00180316,\n",
              "        0.00174832, 0.00173378, 0.0017283 , 0.0016439 , 0.00181341,\n",
              "        0.00178194, 0.00188684, 0.00174141, 0.0017705 , 0.00173545,\n",
              "        0.00180697, 0.00177741, 0.00173235, 0.00183845, 0.0017662 ,\n",
              "        0.00179243, 0.00175071, 0.00178719, 0.00177956, 0.00169539,\n",
              "        0.00172973, 0.00170732, 0.00174046, 0.00196075, 0.00176001,\n",
              "        0.00167441, 0.00172687, 0.00174069, 0.00200081, 0.00182843,\n",
              "        0.00189734, 0.00182033, 0.00169277, 0.00175738, 0.0017271 ,\n",
              "        0.0017128 , 0.00176096, 0.00191188, 0.00178242, 0.00187349,\n",
              "        0.00174308, 0.00160813, 0.0018146 , 0.0019238 , 0.00168371,\n",
              "        0.00188661, 0.00186896, 0.0019381 , 0.00179172, 0.00179386,\n",
              "        0.00187802, 0.00176644, 0.00179172, 0.0017662 , 0.0018692 ,\n",
              "        0.00203395, 0.00172973, 0.00177073, 0.00176692, 0.00175571,\n",
              "        0.00176382, 0.00164032, 0.00176311, 0.00182843, 0.00181246,\n",
              "        0.00178862, 0.00174093, 0.0017724 , 0.00180793, 0.001719  ,\n",
              "        0.00166368, 0.00166106, 0.00156736, 0.00170779, 0.00167179,\n",
              "        0.00178933, 0.00193691, 0.00178289, 0.00166798, 0.00172544,\n",
              "        0.00172138, 0.0022254 , 0.00179601, 0.00178409, 0.00174189,\n",
              "        0.00176859, 0.0017457 , 0.00183344, 0.00174594, 0.00178695,\n",
              "        0.00183296, 0.00180745, 0.00174379, 0.00170112, 0.00175571,\n",
              "        0.00172901, 0.0018084 , 0.00174046, 0.00179124, 0.00170493,\n",
              "        0.00173163, 0.00195909, 0.00183487, 0.00184751, 0.00178885,\n",
              "        0.00166535, 0.00174665, 0.00175261, 0.00166011, 0.00501084,\n",
              "        0.0017221 , 0.00172424, 0.00174928, 0.00166845, 0.00171137,\n",
              "        0.00162983, 0.00167274, 0.00174403, 0.0017333 , 0.00171089,\n",
              "        0.00182104, 0.00182414, 0.00174809, 0.00171423, 0.00168681,\n",
              "        0.00175023, 0.00176239, 0.00185966, 0.00165486, 0.00158691,\n",
              "        0.00171733, 0.00168204, 0.00170517, 0.00173926, 0.0016036 ,\n",
              "        0.00186777, 0.0017705 , 0.00168943, 0.00177455, 0.00169945,\n",
              "        0.00191379, 0.00167203, 0.00179911, 0.00180387, 0.00174665,\n",
              "        0.0017848 , 0.00189137, 0.00192451, 0.00187373, 0.00169635,\n",
              "        0.00176835, 0.0016942 , 0.00185561, 0.00188422, 0.00179696,\n",
              "        0.00166368, 0.00194359, 0.00186276, 0.00174904, 0.00177979,\n",
              "        0.00179601, 0.0017848 , 0.00174451, 0.00178027, 0.00176573,\n",
              "        0.00187254, 0.00177741, 0.00193214, 0.00180411, 0.00180578,\n",
              "        0.00216818, 0.00172281, 0.0018847 , 0.00183654, 0.00177073,\n",
              "        0.00170851, 0.00173426, 0.00186062, 0.00162268, 0.00156713,\n",
              "        0.00180721, 0.00173068, 0.00184488, 0.001683  , 0.00176454,\n",
              "        0.00184631, 0.00174642, 0.00185418, 0.00196719, 0.00170493,\n",
              "        0.00167727, 0.00172019, 0.00167131, 0.00183678, 0.00191522,\n",
              "        0.00174475, 0.00180626, 0.00187588, 0.00190067, 0.00183392,\n",
              "        0.00171995, 0.00177336, 0.00179172, 0.00175691, 0.0018332 ,\n",
              "        0.00188208, 0.00191784, 0.00179291, 0.00183821, 0.00200438,\n",
              "        0.00179625, 0.00179052, 0.00189471, 0.00179863, 0.00204945,\n",
              "        0.00178576, 0.00190711, 0.00177407, 0.00210857, 0.00185394,\n",
              "        0.00189781, 0.00162315, 0.00172091, 0.00193143, 0.00175548,\n",
              "        0.00167036, 0.00167108, 0.00202441, 0.00191879, 0.00182605,\n",
              "        0.00189614, 0.00190258, 0.0017941 , 0.00173664, 0.00180387,\n",
              "        0.00173473, 0.0017066 , 0.00199437, 0.00183272, 0.00191855,\n",
              "        0.00185776, 0.00183082, 0.00178313, 0.00183463, 0.00185466,\n",
              "        0.00182271, 0.0018363 , 0.00184631, 0.00180316, 0.00183606,\n",
              "        0.00181961, 0.00175333, 0.0017271 , 0.00182939, 0.00187802,\n",
              "        0.0019033 , 0.00176525, 0.00167012, 0.00177813, 0.00189042,\n",
              "        0.00184107, 0.00181055, 0.00183725, 0.00172734, 0.00221562,\n",
              "        0.00167108, 0.00180459, 0.00179386, 0.00168753, 0.00160575,\n",
              "        0.00172019, 0.00170827, 0.00174737, 0.00177503, 0.00169921,\n",
              "        0.00167704, 0.00178099, 0.0017705 , 0.00180316, 0.00171614,\n",
              "        0.00175762, 0.00198555, 0.00197554, 0.00212145, 0.00192142,\n",
              "        0.00186348, 0.00174546, 0.00166345, 0.00183415, 0.00179648,\n",
              "        0.00184464, 0.00169587, 0.00171757, 0.00186062, 0.00187111,\n",
              "        0.00197577, 0.00176907, 0.00190854, 0.00183892, 0.00180507,\n",
              "        0.00174689, 0.00175381, 0.00185013, 0.00179625, 0.00178289,\n",
              "        0.00180984, 0.00172639, 0.00165844, 0.0018599 , 0.00166845,\n",
              "        0.00172448, 0.00287032, 0.00176525, 0.00180125, 0.00174952,\n",
              "        0.00171161, 0.00174427, 0.00169802, 0.00167966, 0.00188756,\n",
              "        0.0018456 , 0.0017066 , 0.00186396, 0.00184011, 0.00180292,\n",
              "        0.00178862, 0.00163841, 0.0017488 , 0.00166535, 0.00172424,\n",
              "        0.00178266, 0.00185347, 0.00178933, 0.00171685, 0.00179648,\n",
              "        0.00183177, 0.00197458, 0.00174403, 0.00171852, 0.00182319,\n",
              "        0.00186658, 0.00178742, 0.00177622, 0.00178838, 0.00169516,\n",
              "        0.00177646, 0.00174212, 0.00183082, 0.00193501, 0.0039494 ,\n",
              "        0.00184393, 0.00185108, 0.00352192, 0.00196576, 0.00183558,\n",
              "        0.00176024, 0.00228548, 0.00189495, 0.00178623, 0.00181389,\n",
              "        0.0017643 , 0.00177789, 0.00195241, 0.00178313, 0.00173116,\n",
              "        0.00202036, 0.00183487, 0.00188375, 0.00184751, 0.00174332,\n",
              "        0.0017314 , 0.00185275, 0.00177956, 0.00182748, 0.00183558,\n",
              "        0.0018096 , 0.00183702, 0.00200987, 0.00179029, 0.00179458,\n",
              "        0.00174403, 0.00176001, 0.00168967, 0.00208688, 0.00174713,\n",
              "        0.00167894, 0.00172305, 0.00187016, 0.00190353, 0.00193429,\n",
              "        0.00167561, 0.00187469, 0.00174332, 0.00172949, 0.00177932,\n",
              "        0.00188971, 0.00186419, 0.0017662 , 0.00174022, 0.00183702,\n",
              "        0.00187182, 0.00191975, 0.00167108, 0.00171566, 0.00183058,\n",
              "        0.00186133, 0.00186396, 0.00191474, 0.00174618, 0.00172544,\n",
              "        0.00177789, 0.0017581 , 0.00185966, 0.00188231, 0.00166178,\n",
              "        0.00192356, 0.00180268, 0.00179291, 0.00181508, 0.00179362,\n",
              "        0.00173807, 0.00179863, 0.00180674, 0.00182676, 0.0018034 ,\n",
              "        0.00174618, 0.00182557, 0.00176167, 0.00178576, 0.00171423,\n",
              "        0.0018003 , 0.00181365, 0.00183988, 0.00171733, 0.0017643 ,\n",
              "        0.00190902, 0.00165796, 0.00193644, 0.00175142, 0.00205255,\n",
              "        0.00171804, 0.00173211, 0.00185275, 0.00177503, 0.00187898,\n",
              "        0.00189686, 0.00185847, 0.00186133, 0.00172734, 0.0017972 ,\n",
              "        0.00177765, 0.00180268, 0.00181985, 0.0018661 , 0.00171781,\n",
              "        0.0018301 , 0.00184965, 0.00181174, 0.00189042, 0.00158882,\n",
              "        0.00164962, 0.00177813, 0.00174189, 0.00184488, 0.00176859,\n",
              "        0.00179338, 0.0018518 , 0.00177455, 0.00168633, 0.00198197,\n",
              "        0.00181174, 0.00193787, 0.00174356, 0.00174284, 0.00171423,\n",
              "        0.00177622, 0.00177455, 0.00177479, 0.00179625, 0.00166583,\n",
              "        0.00181079, 0.00169516, 0.00173354, 0.00180221, 0.00173593,\n",
              "        0.00176167, 0.00195479, 0.00183725, 0.00170088, 0.0017488 ,\n",
              "        0.00175023, 0.00173736, 0.00175309]),\n",
              " 'test_accuracy': array([1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
              "        1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
              "        1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
              "        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
              "        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
              "        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
              "        1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
              "        1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
              "        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
              "        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
              "        0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
              "        0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
              "        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
              "        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
              "        0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
              "        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
              "        1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
              "        1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
              "        0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
              "        1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
              "        1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
              "        1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
              "        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
              "        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
              "        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
              "        1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
              "        0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
              "        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
              "        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
              "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
              "        1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
              "        0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
              "        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
              "        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
              "        1., 1., 1.])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VXdvPbqgh6F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d2ce70fc-554d-4860-d22b-119029069980"
      },
      "source": [
        "cv_results['test_accuracy'].mean()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7044270833333334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh7CwmWygh6b",
        "colab_type": "text"
      },
      "source": [
        "We have not included precision and recall in the metrics here. Can you think why?  \n",
        "**<mark>Hint:</mark> Imagine the confusion matrix when the testing has only one sample**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1GyHfC0gh6j",
        "colab_type": "text"
      },
      "source": [
        "## 3. Hyperparameter Tuning\n",
        "Hyperparameters are important parts of the ML model and can make the model gold or trash. Here we have discussed one of the popular hyperparameter tunning method i.e. using Grid Search CV. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEHcCWcegh6l",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 Grid Search CV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQwxdMUkgh6m",
        "colab_type": "text"
      },
      "source": [
        "### 3.1.1 Crime Rate- Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qVEuJimgh6n",
        "colab_type": "text"
      },
      "source": [
        "**Predictor Variable: Crime Rate (Regression Based)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Wbh15ZTgh6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NamrnJjOgh6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "crime = pd.read_csv(\"https://raw.githubusercontent.com/dphi-official/ML_Models/master/Performance_Evaluation/Standard%20Metropolitan%20Areas%20Data%20-%20train_data.csv\")\n",
        "train, test = train_test_split(crime)\n",
        "x_train = train.iloc[:,:-1]     # one can also do train.drop('crime_rate', axis = 1)\n",
        "y_train = train.crime_rate      # or np.array(train.crime_rate.values).reshape(len(x_train),1)\n",
        "x_test = test.iloc[:,:-1]       # or test.drop('crime_rate', axis = 1)\n",
        "y_test = test.crime_rate        # np.array(test.crime_rate.values).reshape(len(x_test),1)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DZvhZ0fgh66",
        "colab_type": "text"
      },
      "source": [
        "Performance without grid search: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TjevVRegh67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "y_pred = lr.predict(x_test)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2rzRQoBgh7D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae3653e5-01e7-4588-c44a-4b5895623e7c"
      },
      "source": [
        "mean_squared_error(y_test, y_pred, squared=False)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.433765668815223"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no8Vw1Qmgh7N",
        "colab_type": "text"
      },
      "source": [
        "Performance with Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UPswE7Ogh7O",
        "colab_type": "text"
      },
      "source": [
        "**Step 1:** Define a parameter Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6084pRAvgh7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False], 'n_jobs':[-1,1,10,15]}"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPQl8cdHgh7Y",
        "colab_type": "text"
      },
      "source": [
        "**Step 2:** Fit the model to find the best hyperparameters on training data, and select the scorer you want to select to optimise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSWCh9rDgh7Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "ba746062-6e48-45e4-e35a-a2aa5a92978e"
      },
      "source": [
        "grid = GridSearchCV(lr,parameters, cv=3)\n",
        "grid.fit(x_train, y_train)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=3, error_score=nan,\n",
              "             estimator=LinearRegression(copy_X=True, fit_intercept=True,\n",
              "                                        n_jobs=None, normalize=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'copy_X': [True, False],\n",
              "                         'fit_intercept': [True, False],\n",
              "                         'n_jobs': [-1, 1, 10, 15],\n",
              "                         'normalize': [True, False]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTdZBb40gh7d",
        "colab_type": "text"
      },
      "source": [
        "**Step 3:** Print the best obtained parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIonXbIogh7f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae337f3e-ee27-474b-94fd-ee1002925ed4"
      },
      "source": [
        "grid.best_estimator_"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Urv2jtngh7n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c97fa79-6ddd-4e64-8141-ae45cb3e083f"
      },
      "source": [
        "grid_lr = LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=False)\n",
        "grid_lr.fit(x_train, y_train)\n",
        "y_pred= grid_lr.predict(x_test)\n",
        "mean_squared_error(y_test, y_pred, squared=False)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.433765668815223"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwhEjnmGgh7v",
        "colab_type": "text"
      },
      "source": [
        "**Performance does not vary that much!**\n",
        "\n",
        "The number of hyperparameters for Linear Regression is very less. Hence all of them give similar performance (in this specific dataset)\n",
        "\n",
        "Let us try another parameter for which the performance varies a lot!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gp-Po0wgh7w",
        "colab_type": "text"
      },
      "source": [
        "### 3.1.2 Artificial Neural Network\n",
        "In Linear Regression, there are not many parameters to optimise, hence performance may not vary that much. In many other classifiers, there are a number of hyper parameters to tune, so let us see an example of how performance is improved using Grid Search. We take an example of **Artificial Neural Networks.**\n",
        "\n",
        "You need not understand the working behind ANN, so it is okay if you do not understand the parameter grid in detail. Let's just see how the performance improves by applying Grid Search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K28nx24bNTMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use diabetes data\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2W1Ei3Ugh7y",
        "colab_type": "text"
      },
      "source": [
        "**Step 1:** Define a parameter Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX_8tDiSgh7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameter_space = {\n",
        "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'alpha': [0.0001, 0.05],\n",
        "    'learning_rate': ['constant','adaptive'],\n",
        "}"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kcl_8hc6gh79",
        "colab_type": "text"
      },
      "source": [
        "**Step 2:** Fit the model to find the best hyperparameters on training data, and select the scorer you want to select to optimise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc9VN84Ygh7-",
        "colab_type": "text"
      },
      "source": [
        "<blockquote> <i>  This code takes a long time to run, you can either skip running this part and directly just see the printed results, or wait for 10-15 mins for this to run </blockquote>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBeiT6D1gh7_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "2e49e8ed-7aaf-4c22-d343-538d879cde01"
      },
      "source": [
        "mlp_random = GridSearchCV(mlp, parameter_space, scoring = 'accuracy')\n",
        "mlp_random.fit(x_train, y_train)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=None, error_score=nan,\n",
              "             estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
              "                                     batch_size='auto', beta_1=0.9,\n",
              "                                     beta_2=0.999, early_stopping=False,\n",
              "                                     epsilon=1e-08, hidden_layer_sizes=(100,),\n",
              "                                     learning_rate='constant',\n",
              "                                     learning_rate_init=0.001, max_fun=15000,\n",
              "                                     max_iter=1000, momentum=0.9,\n",
              "                                     n_iter_no_change=10,\n",
              "                                     nesterovs_momentum=True, power_t=0.5,\n",
              "                                     random_s...\n",
              "                                     validation_fraction=0.1, verbose=False,\n",
              "                                     warm_start=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'activation': ['tanh', 'relu'],\n",
              "                         'alpha': [0.0001, 0.05],\n",
              "                         'hidden_layer_sizes': [(50, 50, 50), (50, 100, 50),\n",
              "                                                (100,)],\n",
              "                         'learning_rate': ['constant', 'adaptive'],\n",
              "                         'solver': ['sgd', 'adam']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='accuracy', verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8f-duE9gh8J",
        "colab_type": "text"
      },
      "source": [
        "**Step 3:** Print the best obtained parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4bj3qClgh8K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a84f23b6-cf31-47e7-ba1a-83a58b7b25d0"
      },
      "source": [
        "mlp_random.best_params_"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'relu',\n",
              " 'alpha': 0.0001,\n",
              " 'hidden_layer_sizes': (50, 50, 50),\n",
              " 'learning_rate': 'constant',\n",
              " 'solver': 'adam'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmHmS7Nfgh8S",
        "colab_type": "text"
      },
      "source": [
        "**Step 4:** Train your model on these parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2repqpLogh8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp_grid = MLPClassifier(solver='adam', learning_rate='constant', hidden_layer_sizes=(100,), alpha=0.0001, \n",
        "                         activation='tanh',max_iter=2000)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCXtKB2Ugh8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp_grid.fit(x_train, y_train)\n",
        "y_pred = mlp_grid.predict(x_test)\n",
        "acc_tuned = accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZTgmXQogh8e",
        "colab_type": "text"
      },
      "source": [
        "**Comparing with Accuracy from model without hyperparameter tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "H-kojEW-gh8f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c4a7a755-586d-418c-9e6d-e55e640a84be"
      },
      "source": [
        "print(\"Accuracy of Tuned model: \",np.round(acc_tuned,3))\n",
        "print(\"Accuracy of non-Tuned model: \",np.round(acc,3))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Tuned model:  0.695\n",
            "Accuracy of non-Tuned model:  0.688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsGwxzNOgh8h",
        "colab_type": "text"
      },
      "source": [
        "Approximately 5% difference in accuracy!  \n",
        "By including an even more exhaustive grid search, we can improve the performance even further"
      ]
    }
  ]
}