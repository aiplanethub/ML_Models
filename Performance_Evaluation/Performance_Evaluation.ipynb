{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Performance Evaluation.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdPA03q3vRei",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1JuiXnrL99eZzIiRzdtREyLlCYhF6Nc4u?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8-k13W0gh1T",
        "colab_type": "text"
      },
      "source": [
        "# PERFORMANCE EVALUATION AND HYPERPARAMETER TUNING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4yt1psqgh1X",
        "colab_type": "text"
      },
      "source": [
        " In this Notebook, we will learn 3 things:   \n",
        "*  Evaluation metrics\n",
        "*  Cross Validation\n",
        "*  Hyperparameter Tuning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D9DMXXigh1Z",
        "colab_type": "text"
      },
      "source": [
        "# 1. Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WQljMFOgh1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D27uCAFGgh1g",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Classification Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfHsda90gh1h",
        "colab_type": "text"
      },
      "source": [
        "**Importing the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24n1XcdMgh1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "diabetes = pd.read_csv(\"diabetes.txt\", header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbHhT_yMgh1p",
        "colab_type": "text"
      },
      "source": [
        "**This dataset contains 13 columns and based on different features, it is guessed whether or not a person has Diabetes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kYzsmxfgh1p",
        "colab_type": "text"
      },
      "source": [
        "<blockquote> 0 = No Diabetes  <br>\n",
        "1 = Diabetes</blockquote>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64-GjizMgh1q",
        "colab_type": "text"
      },
      "source": [
        " For Each Attribute: (all numeric-valued)\n",
        " <blockquote>\n",
        "   1. Number of times pregnant\n",
        "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
        "   3. Diastolic blood pressure (mm Hg)\n",
        "   4. Triceps skin fold thickness (mm)\n",
        "   5. 2-Hour serum insulin (mu U/ml)\n",
        "   6. Body mass index (weight in kg/(height in m)^2)\n",
        "   7. Diabetes pedigree function\n",
        "   8. Age (years)\n",
        "   9. Class variable (0 or 1)\n",
        "   </blockquote>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lWwrcYVgh1s",
        "colab_type": "code",
        "colab": {},
        "outputId": "b2a36459-641a-4442-ccf0-624ae738fc5a"
      },
      "source": [
        "diabetes.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0    1   2   3    4     5      6   7  8\n",
              "0  6  148  72  35    0  33.6  0.627  50  1\n",
              "1  1   85  66  29    0  26.6  0.351  31  0\n",
              "2  8  183  64   0    0  23.3  0.672  32  1\n",
              "3  1   89  66  23   94  28.1  0.167  21  0\n",
              "4  0  137  40  35  168  43.1  2.288  33  1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOQVck62gh1z",
        "colab_type": "text"
      },
      "source": [
        "#### Split into training and testing (80:20)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AylKRHMgh12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(diabetes.iloc[:, :-1], diabetes.iloc[:,-1], test_size=0.2, random_state=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3R17-E7kqjD",
        "colab_type": "text"
      },
      "source": [
        "**Note for learners:** Here we have used MLPClassifier from neural_network module of sklearn library. MLP Classifier is also a classification algorithm like logistic regression or decision tree. We will soon learn about Neural Networks and Artificial Neural Networks in the upcoming sessions. So, no need to worry about it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZizeBIHwgh17",
        "colab_type": "text"
      },
      "source": [
        "### Developing a model and checking it's performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqeygkPIgh18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "mlp = MLPClassifier(max_iter=1000)\n",
        "mlp.fit(x_train, y_train)\n",
        "y_pred = mlp.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL5FiSE_gh2F",
        "colab_type": "text"
      },
      "source": [
        "## Performance Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD9O15axgh2G",
        "colab_type": "text"
      },
      "source": [
        "**All performance metrics in sklearn are to be written in the same way -**  \n",
        "> ``` metric_function(true_label, predicted_labels) ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtZ2yI1Lgh2I",
        "colab_type": "text"
      },
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKr_nFuRgh2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12hGE_5rgh2O",
        "colab_type": "code",
        "colab": {},
        "outputId": "98a360c7-5722-480c-ea27-9864a1e113d7"
      },
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "print(\"True Positive\", tp)\n",
        "print(\"True Negative\", tn)\n",
        "print(\"False Positive\", fp)\n",
        "print(\"False Negative\", fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True Positive 34\n",
            "True Negative 67\n",
            "False Positive 25\n",
            "False Negative 28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqjqAVSQgh2S",
        "colab_type": "text"
      },
      "source": [
        "### Accuracy\n",
        "\\begin{align}\n",
        "Accuracy = \\frac{TP+TN}{TP+TN+FN+FP}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AX3DELNUgh2U",
        "colab_type": "code",
        "colab": {},
        "outputId": "f7da71ff-aea6-4980-cdd5-0ad2727c866c"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6558441558441559"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh4fSgV9gh2a",
        "colab_type": "text"
      },
      "source": [
        "### Recall (Sensitivity)\n",
        "\\begin{align}\n",
        "Sensitivity = \\frac{TP}{TP+FN}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGBdDw9hgh2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import recall_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk9ffog9gh2g",
        "colab_type": "code",
        "colab": {},
        "outputId": "a9c097de-131e-4bd8-e146-d3dcce7fe27a"
      },
      "source": [
        "recall_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5483870967741935"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlQf5BUtgh2o",
        "colab_type": "text"
      },
      "source": [
        "### Specificity\n",
        "sklearn does not have an inbuild function for Specificity. But by adding parameter pos_label =0 to the recall function, we treat that as the positive class, and hence gives the correct output\n",
        "\\begin{align}\n",
        "Specificity = \\frac{TN}{TN+FP}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHTxK6NHgh2q",
        "colab_type": "code",
        "colab": {},
        "outputId": "317ea1c1-5c5d-4016-c66a-e5ab9eb78cfb"
      },
      "source": [
        "print(\"Specificity with recall pos label=0: \",recall_score(y_test, y_pred, pos_label=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Specificity with recall pos label=0:  0.7282608695652174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UAM0Kz2gh2u",
        "colab_type": "text"
      },
      "source": [
        "**Checking with formulas (tn , fp from confusion matrix):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrthV4dhgh2v",
        "colab_type": "code",
        "colab": {},
        "outputId": "0cf93298-77ec-4e74-cc03-ab6055bc2c25"
      },
      "source": [
        "print(\"Specificity with Formulas: \", tn/(tn+fp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Specificity with Formulas:  0.7282608695652174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7QrITQBgh21",
        "colab_type": "text"
      },
      "source": [
        "They are the same! You can use either one of them!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plCwrQB_gh23",
        "colab_type": "text"
      },
      "source": [
        "### Precision\n",
        "\\begin{align}\n",
        "Precision = \\frac{TP}{TP+FP}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14pynqe6gh25",
        "colab_type": "code",
        "colab": {},
        "outputId": "20c560ea-1a75-4c8a-bbf0-30f7ffbecf73"
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "precision_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.576271186440678"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_2Y9vQ9gh2_",
        "colab_type": "text"
      },
      "source": [
        "### Imbalanced Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwMwh4mhgh3A",
        "colab_type": "code",
        "colab": {},
        "outputId": "50987b64-c52b-4333-fc56-123d0cc791b3"
      },
      "source": [
        "diabetes.iloc[:,-1].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    500\n",
              "1    268\n",
              "Name: 8, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY7eJaxsgh3K",
        "colab_type": "text"
      },
      "source": [
        "### Matthews Correlation Coefficient\n",
        "\\begin{align}\n",
        "MCC = \\frac{(TP*TN)-(FP*FN)}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vr62zEHWgh3L",
        "colab_type": "code",
        "colab": {},
        "outputId": "af7b306d-ebd0-4518-de4b-96eea5d57625"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "print(\"MCC Score: \",matthews_corrcoef(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC Score:  0.27908046118474855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hum7bVaAgh3P",
        "colab_type": "text"
      },
      "source": [
        "### F1 Score\n",
        "It is the harmonic mean of Precision and recall\n",
        "\n",
        "\\begin{align}\n",
        "Precision = \\frac{2*Precision*Recall}{Precision+Recall}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3A9uIKwgh3Q",
        "colab_type": "code",
        "colab": {},
        "outputId": "04d30b00-fae4-46d5-d247-997e8c743574"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "print(\"F1 Score: \",f1_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 Score:  0.5619834710743801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVcVsBEhgh3V",
        "colab_type": "text"
      },
      "source": [
        "## Area Under the Curve (Reciever Operating Characterstics)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YypfoTaVgh3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import plot_roc_curve"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZPwqn0Dgh3c",
        "colab_type": "code",
        "colab": {},
        "outputId": "936a73a9-ef34-45a9-d7fa-88af9ad489f4"
      },
      "source": [
        "plot_roc_curve(mlp, x_test, y_test)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHrohQufgh3l",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Regression Metrics "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUc_2Fz7gh3m",
        "colab_type": "text"
      },
      "source": [
        "Wine Dataset  \n",
        "    <b> Predictor Variable: </b> Quality (Tells quality of wine)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMZbaFOcgh3o",
        "colab_type": "code",
        "colab": {},
        "outputId": "6db161d7-209b-480e-b626-1e8915114fe1"
      },
      "source": [
        "wine = pd.read_csv(\"winequality.csv\", sep=\";\")\n",
        "wine.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.0</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.36</td>\n",
              "      <td>20.7</td>\n",
              "      <td>0.045</td>\n",
              "      <td>45.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>1.0010</td>\n",
              "      <td>3.00</td>\n",
              "      <td>0.45</td>\n",
              "      <td>8.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.3</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.34</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.049</td>\n",
              "      <td>14.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.9940</td>\n",
              "      <td>3.30</td>\n",
              "      <td>0.49</td>\n",
              "      <td>9.5</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.1</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.40</td>\n",
              "      <td>6.9</td>\n",
              "      <td>0.050</td>\n",
              "      <td>30.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0.9951</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.44</td>\n",
              "      <td>10.1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.5</td>\n",
              "      <td>0.058</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.5</td>\n",
              "      <td>0.058</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
              "0            7.0              0.27         0.36            20.7      0.045   \n",
              "1            6.3              0.30         0.34             1.6      0.049   \n",
              "2            8.1              0.28         0.40             6.9      0.050   \n",
              "3            7.2              0.23         0.32             8.5      0.058   \n",
              "4            7.2              0.23         0.32             8.5      0.058   \n",
              "\n",
              "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
              "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
              "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
              "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
              "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
              "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
              "\n",
              "   alcohol  quality  \n",
              "0      8.8        6  \n",
              "1      9.5        6  \n",
              "2     10.1        6  \n",
              "3      9.9        6  \n",
              "4      9.9        6  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP8EUDXTgh3s",
        "colab_type": "text"
      },
      "source": [
        "#### Split into training and testing (80:20)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGgMF3Rzgh3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(wine.iloc[:, :-1], wine.iloc[:,-1], test_size=0.2, random_state=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnuT2aaQgh3z",
        "colab_type": "text"
      },
      "source": [
        "Creating a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsRa3Pe0gh32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PMAIOYRgh4B",
        "colab_type": "code",
        "colab": {},
        "outputId": "3a934fd8-0598-4e75-fa61-fd6f7d35e887"
      },
      "source": [
        "lr = LinearRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "y_pred = lr.predict(x_test)\n",
        "y_pred[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.44455619, 5.57868309, 5.99091469, 5.19864346, 6.0666099 ,\n",
              "       5.01639077, 5.68416174, 6.26611011, 5.97010538, 5.65519351])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEQuSCzMgh4R",
        "colab_type": "text"
      },
      "source": [
        "## Performance Measurement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FZ86Xpbgh4U",
        "colab_type": "text"
      },
      "source": [
        "Let y = Actual Value,  $\\tilde{y}$ = Predicted Value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjIBhaItgh4X",
        "colab_type": "text"
      },
      "source": [
        "### Mean Absolute Error  \n",
        "*  MAE is the absolute difference between the target value and the value predicted by the model.  \n",
        "*  The MAE is more robust to outliers and does not penalize the errors as extremely as mse\n",
        "\\begin{align}\n",
        "MAE  = \\frac{1}{n}\\sum|y-\\tilde{y}| \n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNOb1yb2gh4Y",
        "colab_type": "code",
        "colab": {},
        "outputId": "8b676dcb-3d46-4989-def3-ca57606f863b"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "mean_absolute_error(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5972358558776466"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHrz_JZAgh4j",
        "colab_type": "text"
      },
      "source": [
        "### Mean Squared Error\n",
        "*  It is simply the average of the squared difference between the target value and the value predicted by the regression model. \n",
        "*  As it squares the differences, it penalizes even a small error which leads to over-estimation of how bad the model is.\n",
        "*  MSE or Mean Squared Error is one of the most preferred metrics for regression tasks. \n",
        "\\begin{align}\n",
        "MSE & = \\frac{1}{n}\\sum(y-\\tilde{y})^2\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHZcMiDrgh4k",
        "colab_type": "code",
        "colab": {},
        "outputId": "5dd51735-40c5-4391-8277-d045487e3986"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"Mean Squared Error: \",mean_squared_error(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Squared Error:  0.5906658099548074\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx7Dtz5Wgh4r",
        "colab_type": "text"
      },
      "source": [
        "### Root Mean Square Error\n",
        "*  RMSE is the square root of the averaged squared difference between the target value and the value predicted by the model. \n",
        "*  It is preferred more in some cases because the errors are first squared before averaging which poses a high penalty on large errors.\n",
        "*  This implies that RMSE is useful when large errors are undesired.\n",
        "\\begin{align}\n",
        "RMSE  = \\sqrt{\\frac{1}{n}\\sum(y-\\tilde{y})^2}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "EfQI1JFzgh4u",
        "colab_type": "code",
        "colab": {},
        "outputId": "1a13455a-6d96-4d1f-c31c-08d138efe033"
      },
      "source": [
        "print(\"Root Mean Squared Error: \",mean_squared_error(y_test, y_pred, squared=False))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Root Mean Squared Error:  0.7685478579469254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Onetrj_Agh5A",
        "colab_type": "text"
      },
      "source": [
        "### R Squared\n",
        "<li>The metric helps us to compare our current model with a constant baseline and tells us how much our model is better\n",
        "\\begin{align}\n",
        "R^2 = 1 - \\frac{MSE(Model)}{MSE(Baseline)}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmTpqG2rgh5B",
        "colab_type": "code",
        "colab": {},
        "outputId": "c4094d27-ae63-4f90-82b4-b937fac4bbc4"
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.28320371911110254"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VoVhiVKgh5H",
        "colab_type": "text"
      },
      "source": [
        "# 2. Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gESg0k8_gh5H",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 K-Fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKX8Gjyugh5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_validate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7kl2Ah_gh5Q",
        "colab_type": "text"
      },
      "source": [
        "<blockquote> We can also import cross_val_score from the same library, but it only allows a single scorer to be implemented. So we are using cross_validate </blockquote>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GiU8TBWgh5Q",
        "colab_type": "code",
        "colab": {},
        "outputId": "9135426a-5c1e-41d1-f55c-5b71236672d9"
      },
      "source": [
        "cv_results = cross_validate(mlp,diabetes.iloc[:, :-1], diabetes.iloc[:,-1], cv=10, scoring=[\"accuracy\", \"precision\", \"recall\"])\n",
        "cv_results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fit_time': array([0.25549912, 0.24463391, 0.3375051 , 0.38997602, 0.43451691,\n",
              "        0.28148103, 0.42256188, 0.46015286, 0.33547425, 0.31083918]),\n",
              " 'score_time': array([0.00319004, 0.00313807, 0.0065639 , 0.00598717, 0.00640011,\n",
              "        0.00310993, 0.00333023, 0.0031321 , 0.00328565, 0.0031507 ]),\n",
              " 'test_accuracy': array([0.68831169, 0.74025974, 0.7012987 , 0.67532468, 0.67532468,\n",
              "        0.77922078, 0.76623377, 0.7012987 , 0.69736842, 0.43421053]),\n",
              " 'test_precision': array([0.53846154, 0.68421053, 0.57692308, 0.53846154, 0.6       ,\n",
              "        0.66666667, 0.69565217, 0.54545455, 0.61538462, 0.3559322 ]),\n",
              " 'test_recall': array([0.77777778, 0.48148148, 0.55555556, 0.51851852, 0.22222222,\n",
              "        0.74074074, 0.59259259, 0.88888889, 0.30769231, 0.80769231])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2N7yzQRgh5V",
        "colab_type": "text"
      },
      "source": [
        "**cv=10 is provided, which means we are performing 10 fold cross validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxN-rYmigh5W",
        "colab_type": "code",
        "colab": {},
        "outputId": "6adccae4-b52f-486b-a0a2-45ab9290ea48"
      },
      "source": [
        "print(\"Accuracy: \", cv_results[\"test_accuracy\"].mean())\n",
        "print(\"Precision: \", cv_results[\"test_precision\"].mean())\n",
        "print(\"Recall: \", cv_results[\"test_recall\"].mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.6858851674641149\n",
            "Precision:  0.5817146884970645\n",
            "Recall:  0.5893162393162392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrNp0R93gh5a",
        "colab_type": "text"
      },
      "source": [
        "**For all valid scoring options - use the following:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mz4G3U5gh5e",
        "colab_type": "code",
        "colab": {},
        "outputId": "71c6e6bb-79b5-4186-8375-89852a9c4203"
      },
      "source": [
        "import sklearn.metrics as m\n",
        "m.SCORERS.keys()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygp5tnDdgh5q",
        "colab_type": "text"
      },
      "source": [
        "For more complicated scoring metrics (such as specificity, which isn't explicilty provided by sklearn), or to create your own metrics, \n",
        "http://scikit-learn.org/stable/modules/model_evaluation.html#using-multiple-metric-evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoVnDQsigh5x",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Leave One Out Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDxbxH6ggh5x",
        "colab_type": "text"
      },
      "source": [
        "<blockquote>This code takes a long time to run, you can either skip running this part and directly just see the printed results, or wait for 10-15 mins for this to run </blockquote>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZEbpVaygh5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import LeaveOneOut"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "vBvtai8Xgh55",
        "colab_type": "code",
        "colab": {},
        "outputId": "e3d7e547-afab-4ab6-dac0-9de3db21ad43"
      },
      "source": [
        "cv_results = cross_validate(mlp,diabetes.iloc[:, :-1], diabetes.iloc[:,-1],\n",
        "                            cv=loocv, scoring=[\"accuracy\"])\n",
        "cv_results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fit_time': array([0.40177393, 0.63890791, 0.66640902, 0.61088014, 0.59156919,\n",
              "        0.59804511, 0.30579686, 0.36277413, 0.35784912, 0.36950803,\n",
              "        0.65370393, 0.55987096, 0.59508395, 0.4965179 , 0.52798319,\n",
              "        0.50680494, 0.64666176, 0.59816122, 0.53342104, 0.36651707,\n",
              "        0.53289104, 0.50388098, 0.51609015, 0.51078796, 0.52557516,\n",
              "        0.49945879, 0.43741512, 0.42034721, 0.46922088, 0.47810483,\n",
              "        0.44937706, 0.512784  , 0.43379879, 0.42749691, 0.41925883,\n",
              "        0.43691182, 0.4203949 , 0.51269197, 0.49413514, 0.51307273,\n",
              "        0.50170922, 0.51180124, 0.49824095, 0.51085901, 0.49622726,\n",
              "        0.51620388, 0.54484606, 0.58760595, 0.56213808, 0.58630776,\n",
              "        0.55719781, 0.62100196, 0.62324595, 0.67511296, 2.11149216,\n",
              "        0.59618735, 0.46033096, 0.41720009, 1.30450583, 0.51683187,\n",
              "        0.3933382 , 0.61483192, 0.83597779, 3.26361704, 1.18034673,\n",
              "        0.981318  , 3.38164687, 2.01949692, 2.51855016, 6.07708287,\n",
              "        2.77653289, 2.352494  , 2.42621493, 2.12061524, 1.7454071 ,\n",
              "        2.38935614, 2.2390368 , 7.5503912 , 2.85718393, 1.87032604,\n",
              "        1.07391524, 1.78771782, 1.03489494, 1.73067713, 2.20467281,\n",
              "        1.48690677, 2.12218285, 1.10259604, 1.35316491, 1.11633205,\n",
              "        0.81752419, 0.75911617, 0.84298825, 0.87715697, 0.90369368,\n",
              "        0.89351296, 0.68232274, 0.66420889, 0.68191099, 0.63747978,\n",
              "        0.83674216, 0.81091809, 0.87575793, 1.12301493, 0.88447595,\n",
              "        0.86999583, 0.79535198, 0.85208297, 0.62404895, 0.57656288,\n",
              "        0.73319387, 0.56595731, 0.52309299, 0.54078102, 0.66422415,\n",
              "        0.64031792, 0.57076383, 0.54870391, 0.9918921 , 0.53410411,\n",
              "        0.52678394, 0.54929686, 0.53125119, 0.54604292, 0.55187726,\n",
              "        0.51142693, 0.58895397, 0.58031106, 0.54605222, 0.51358795,\n",
              "        0.60706925, 0.53807807, 0.50773811, 0.54132199, 0.54364705,\n",
              "        0.56621122, 0.58594203, 0.58555198, 0.57967997, 0.58915997,\n",
              "        0.60540271, 0.59840894, 0.75317311, 0.55755019, 0.51918697,\n",
              "        0.538831  , 0.52979207, 0.53767276, 0.50450706, 0.53669405,\n",
              "        0.50974107, 0.52690482, 0.50838089, 0.59717727, 0.71234727,\n",
              "        0.52104902, 0.52084303, 0.62263608, 0.59049177, 0.51481485,\n",
              "        0.51880908, 0.51733184, 0.55595279, 0.52387714, 0.60096002,\n",
              "        0.7250371 , 0.87686181, 0.59885192, 0.67498708, 1.25271297,\n",
              "        0.62804413, 0.86588907, 0.71642804, 0.91324186, 0.96699524,\n",
              "        0.70430493, 0.53039074, 0.47823286, 0.53857303, 0.51799917,\n",
              "        0.61182094, 0.61941385, 0.51979589, 0.4757421 , 0.49531817,\n",
              "        0.47648001, 0.47796893, 0.52368617, 0.53228974, 0.52226615,\n",
              "        0.55039525, 0.52581811, 0.50151801, 0.59330297, 0.59204888,\n",
              "        0.565799  , 0.49597073, 0.52180409, 0.61473703, 0.50231194,\n",
              "        0.50295615, 0.54275393, 0.50578666, 0.79281712, 0.55202198,\n",
              "        0.47443295, 0.52281976, 0.47427893, 0.48167562, 0.44692302,\n",
              "        0.48168182, 0.46359324, 0.62219191, 0.609869  , 0.48120308,\n",
              "        0.49939203, 0.48004699, 0.47460413, 0.53777885, 0.4793818 ,\n",
              "        0.53518701, 0.53162003, 0.58048511, 0.54015493, 0.60671997,\n",
              "        0.60526109, 0.53551006, 0.54820681, 0.46407723, 0.52919078,\n",
              "        0.78990889, 0.5892117 , 0.57080603, 0.56723118, 0.55832696,\n",
              "        0.5524199 , 0.52795315, 0.52201605, 0.51953292, 0.53012681,\n",
              "        0.51283503, 0.57373691, 0.54521704, 0.57621098, 0.75184584,\n",
              "        0.968606  , 0.60171008, 0.58671284, 0.81871796, 0.84729385,\n",
              "        0.63597703, 0.59222889, 0.51637983, 0.55133319, 0.54752588,\n",
              "        0.62747502, 0.54905891, 0.53295803, 0.53267694, 0.51535511,\n",
              "        0.52596617, 0.53605795, 0.54640317, 0.52006292, 0.47873306,\n",
              "        0.50652719, 0.56358886, 0.50726199, 0.66076708, 0.72207689,\n",
              "        0.50931621, 0.53437972, 0.53278613, 0.53595209, 0.5652349 ,\n",
              "        0.57701898, 0.56765914, 0.58171606, 0.6923542 , 0.63021421,\n",
              "        0.62538981, 0.53467512, 0.51940012, 0.52571702, 0.5577302 ,\n",
              "        0.52939606, 0.72140479, 0.57964897, 0.53703594, 0.50976682,\n",
              "        0.52363801, 0.5567112 , 0.54705095, 0.50430322, 0.55041575,\n",
              "        0.54988599, 0.56282973, 0.52491593, 0.55249596, 0.52440023,\n",
              "        0.53953409, 0.52415395, 0.53667974, 0.45803308, 0.53108311,\n",
              "        0.444098  , 0.53234506, 0.4643321 , 0.44571686, 0.61350083,\n",
              "        0.64414907, 0.65407276, 0.66091585, 0.57161498, 0.55578804,\n",
              "        0.53116894, 0.52720714, 0.51967216, 0.52666593, 0.51005316,\n",
              "        0.51620793, 0.53040624, 0.52490187, 0.69814515, 0.60405993,\n",
              "        0.47165203, 0.50740409, 0.5286479 , 0.54988194, 0.52766109,\n",
              "        0.52214599, 0.5356698 , 0.52271795, 0.55324101, 0.53160596,\n",
              "        0.55383182, 0.61048007, 0.64007878, 0.59196973, 0.53535795,\n",
              "        0.56327987, 0.67727017, 0.63429308, 0.52325606, 0.52089   ,\n",
              "        0.51094508, 0.54172015, 0.57505107, 0.56149793, 0.54946089,\n",
              "        0.59981799, 0.57707596, 0.5212338 , 0.51259184, 0.5156281 ,\n",
              "        0.54501605, 0.51400495, 0.52547908, 0.88804197, 0.76469803,\n",
              "        0.63737583, 0.79060507, 0.78730178, 0.60496593, 0.68506908,\n",
              "        0.60247207, 0.63495803, 0.70985794, 0.59708905, 0.5431931 ,\n",
              "        0.56702089, 0.55056787, 0.52876401, 0.57087016, 0.55985522,\n",
              "        0.55727696, 0.53977394, 0.52537179, 0.53314257, 0.53740096,\n",
              "        0.74558496, 0.64595103, 0.55985594, 0.59886885, 0.55301094,\n",
              "        0.52076697, 0.5196569 , 0.53556609, 0.53022408, 0.55122995,\n",
              "        0.54473591, 0.53417969, 0.63644743, 0.55089402, 0.3037262 ,\n",
              "        0.30306268, 0.53816199, 0.53963995, 0.57076621, 0.73453903,\n",
              "        0.60456491, 0.28798389, 0.54883218, 0.52888799, 0.54575634,\n",
              "        0.53752398, 0.53308392, 0.55396986, 0.55179095, 0.58113003,\n",
              "        0.73750806, 0.59966397, 0.57011199, 0.57372904, 0.55391598,\n",
              "        0.52207303, 0.71219802, 0.72277522, 0.73497319, 0.8404479 ,\n",
              "        0.72222209, 0.71412206, 0.73918891, 0.65793896, 0.6563499 ,\n",
              "        0.67985821, 0.66640091, 0.66661215, 0.93278289, 0.64882874,\n",
              "        0.59909701, 0.86161494, 0.65359521, 0.59479499, 0.57836699,\n",
              "        0.572824  , 0.59744692, 0.60955906, 0.59052801, 0.59914184,\n",
              "        0.68821812, 0.6184001 , 0.63804317, 0.75238824, 0.64269614,\n",
              "        0.63892603, 0.75372005, 0.81940794, 0.64673901, 0.64930797,\n",
              "        0.61350799, 0.49819279, 0.58459902, 0.71149898, 0.54513097,\n",
              "        0.65436578, 0.59841323, 0.51682806, 0.59777093, 0.57303023,\n",
              "        0.58427191, 0.59175396, 0.58820701, 0.59005189, 0.56944895,\n",
              "        0.58421493, 0.62361026, 0.5883832 , 0.81838512, 0.76655412,\n",
              "        0.84083772, 0.95791411, 0.79048705, 0.62897801, 0.6296742 ,\n",
              "        1.00732517, 0.97947717, 0.82501602, 0.75763893, 0.83121395,\n",
              "        0.72091103, 0.41921377, 0.43196321, 0.5424211 , 0.83518696,\n",
              "        0.59063721, 0.64379501, 0.76505709, 0.51347208, 0.58692694,\n",
              "        0.68450403, 0.43648267, 0.42387605, 0.44154   , 0.57028699,\n",
              "        0.39970112, 0.31817317, 0.27010298, 0.40884113, 0.49375582,\n",
              "        0.44161201, 0.43386889, 0.61705017, 0.46869898, 0.67992115,\n",
              "        0.62058091, 0.50452805, 0.46521688, 0.32330894, 0.31529975,\n",
              "        0.33292413, 0.3184979 , 0.68797469, 0.41286206, 0.39616799,\n",
              "        0.68990684, 0.35770011, 0.33645105, 0.50952315, 0.31893706,\n",
              "        0.32148314, 0.32431293, 0.3422358 , 0.32679915, 0.32047606,\n",
              "        0.32751894, 0.33218884, 0.31796885, 0.32931519, 0.3537991 ,\n",
              "        0.39884496, 0.42810702, 0.36823893, 0.50103903, 0.43367481,\n",
              "        0.33447099, 0.32134223, 0.31857681, 0.38445401, 0.305794  ,\n",
              "        0.31984282, 0.32073092, 0.3237381 , 0.845927  , 0.31009603,\n",
              "        0.67539525, 0.67561817, 0.31088209, 0.52408695, 0.6978581 ,\n",
              "        0.32416511, 0.72242403, 0.54085398, 0.31492496, 0.33784103,\n",
              "        0.33145118, 0.30939507, 0.31610703, 0.33402967, 0.31930804,\n",
              "        0.3131237 , 0.35925484, 0.35073209, 0.33908582, 0.35387492,\n",
              "        0.3391099 , 0.34137702, 0.3717351 , 0.39333415, 0.3728528 ,\n",
              "        0.37999082, 0.38327479, 0.33290792, 0.33694506, 0.32595515,\n",
              "        0.3339467 , 0.32801056, 0.31604409, 0.32122493, 0.31381798,\n",
              "        0.32264018, 0.31600904, 0.32371116, 0.3293469 , 0.32072186,\n",
              "        0.43038893, 0.43852592, 0.35297704, 0.30769801, 0.59602308,\n",
              "        0.30737424, 0.26888299, 0.27380919, 0.64380121, 0.42645717,\n",
              "        0.63908982, 0.2852838 , 0.63963389, 0.45631576, 0.46524692,\n",
              "        0.4378531 , 0.64407706, 0.47196698, 0.51068521, 0.30335784,\n",
              "        0.29404521, 0.27725482, 0.28292394, 0.2870822 , 0.286026  ,\n",
              "        0.28148007, 0.28659511, 0.29166198, 0.27919793, 0.30691504,\n",
              "        0.43621302, 0.71675897, 0.29215407, 0.68318892, 0.29462218,\n",
              "        0.27969527, 0.27003193, 0.27315307, 0.28350329, 0.27750492,\n",
              "        0.27202106, 0.28014421, 0.26912999, 0.44198513, 0.29189682,\n",
              "        0.27055287, 0.27162504, 0.51111674, 0.49785304, 0.34832501,\n",
              "        0.314219  , 0.33044815, 0.32986808, 0.34723115, 0.38938594,\n",
              "        0.36319518, 0.299088  , 0.30875421, 0.35977006, 0.27048087,\n",
              "        0.27165794, 0.27594614, 0.27853894, 0.2696929 , 0.27037191,\n",
              "        0.27870417, 0.28799796, 0.27962208, 0.78632498, 0.34112597,\n",
              "        0.34517813, 0.35806179, 0.42830205, 0.31793714, 0.39406705,\n",
              "        0.36525607, 0.36915803, 0.55067706, 0.46946597, 0.58204913,\n",
              "        0.48357296, 0.45174289, 0.60573483, 0.39493704, 0.599123  ,\n",
              "        0.35307002, 0.59419799, 0.59522486, 0.68166614, 0.44758797,\n",
              "        0.37830687, 0.64352393, 0.43771529, 0.32707405, 0.33491802,\n",
              "        0.34541392, 0.33664322, 0.32726073, 0.32138586, 0.32443094,\n",
              "        0.33827186, 0.32445598, 0.34980416, 0.33461809, 0.33156681,\n",
              "        0.35522676, 0.33668876, 0.3269012 , 0.33442211, 0.50498915,\n",
              "        0.94434786, 0.6298089 , 0.51136613, 0.3832767 , 0.36260819,\n",
              "        0.38722682, 0.45647287, 0.48270893, 0.51966476, 0.37444782,\n",
              "        0.37081695, 0.3345468 , 0.48258924, 0.32676697, 0.33600211,\n",
              "        0.39230323, 0.333318  , 0.46476912, 0.65190196, 0.39476299,\n",
              "        0.44576001, 0.42701221, 0.34662342, 0.3516717 , 0.34627509,\n",
              "        0.34634018, 0.36055613, 0.34471917, 0.36283183, 0.3290081 ,\n",
              "        0.40234399, 0.33567595, 0.3182652 , 0.31762505, 0.46353889,\n",
              "        0.43569207, 0.34999704, 0.2428453 , 0.33386302, 0.33813095,\n",
              "        0.35465479, 0.33678627, 0.3250699 , 0.35025692, 0.40830278,\n",
              "        0.39826512, 0.38319325, 0.39470792, 0.32651091, 0.32620215,\n",
              "        0.32850599, 0.39596796, 0.38737512, 0.41355395, 0.38321495,\n",
              "        0.39239597, 0.38617182, 0.40155196, 0.40245104, 0.3719461 ,\n",
              "        0.49575186, 0.63342905, 0.54413295, 0.6934731 , 0.72438407,\n",
              "        0.54086089, 0.33136797, 0.53696823, 0.52367115, 0.56091213,\n",
              "        0.56794691, 0.61034393, 0.58458424]),\n",
              " 'score_time': array([0.00124192, 0.00120807, 0.00158596, 0.00121784, 0.00120878,\n",
              "        0.00119805, 0.0012002 , 0.00120497, 0.00119686, 0.00120783,\n",
              "        0.00120187, 0.00120282, 0.00122094, 0.00120115, 0.00126004,\n",
              "        0.00121403, 0.00165725, 0.00121069, 0.00141382, 0.00124407,\n",
              "        0.00122404, 0.00122809, 0.0012188 , 0.00123191, 0.00121188,\n",
              "        0.00124526, 0.00127888, 0.00121093, 0.00121593, 0.00121617,\n",
              "        0.00121379, 0.00122213, 0.00122118, 0.00121713, 0.00120592,\n",
              "        0.00120902, 0.00122094, 0.0012269 , 0.00120497, 0.00122595,\n",
              "        0.00120115, 0.00121784, 0.00119925, 0.00120282, 0.00120497,\n",
              "        0.00122714, 0.00119996, 0.00120997, 0.00120902, 0.00121212,\n",
              "        0.00120807, 0.00120306, 0.0014112 , 0.00120687, 0.00207686,\n",
              "        0.00354886, 0.00262809, 0.00131297, 0.00215507, 0.00233793,\n",
              "        0.00120282, 0.00227284, 0.00204706, 0.0017221 , 0.00206304,\n",
              "        0.00208092, 0.00182104, 0.00285697, 0.00248694, 0.00277495,\n",
              "        0.00598812, 0.00209475, 0.00262713, 0.00226903, 0.00193596,\n",
              "        0.0018549 , 0.00179911, 0.00230384, 0.0023191 , 0.00179601,\n",
              "        0.00179982, 0.00321198, 0.00193691, 0.00359201, 0.00241017,\n",
              "        0.0021162 , 0.00254703, 0.00163507, 0.00205922, 0.0013411 ,\n",
              "        0.00179005, 0.00205517, 0.0022037 , 0.0028801 , 0.00215816,\n",
              "        0.00141096, 0.00139809, 0.00130105, 0.0030551 , 0.00173998,\n",
              "        0.00124478, 0.00204492, 0.00259519, 0.00145984, 0.00125313,\n",
              "        0.0014081 , 0.00123906, 0.00158787, 0.00123596, 0.00190687,\n",
              "        0.00129509, 0.00122976, 0.00193119, 0.00171113, 0.00124002,\n",
              "        0.00188017, 0.00122118, 0.00126386, 0.00155783, 0.00132775,\n",
              "        0.00287509, 0.00138021, 0.001755  , 0.00166392, 0.00141907,\n",
              "        0.00120711, 0.00181389, 0.00123906, 0.00123191, 0.00139093,\n",
              "        0.00273395, 0.00181103, 0.0017519 , 0.00137711, 0.00148487,\n",
              "        0.00120997, 0.00180793, 0.00220609, 0.00204802, 0.00121093,\n",
              "        0.00187516, 0.00247192, 0.00120568, 0.00175881, 0.00128603,\n",
              "        0.0013361 , 0.00121212, 0.00149298, 0.00120974, 0.00125694,\n",
              "        0.00122094, 0.00208211, 0.00123191, 0.00179362, 0.00131989,\n",
              "        0.00137615, 0.00154591, 0.00121808, 0.00121713, 0.00140119,\n",
              "        0.00133419, 0.00124192, 0.00130796, 0.00193477, 0.00914097,\n",
              "        0.00211883, 0.00121999, 0.00121498, 0.00129414, 0.00213504,\n",
              "        0.00205207, 0.00124407, 0.00177813, 0.00120711, 0.00192785,\n",
              "        0.00137496, 0.00139999, 0.00121117, 0.00207782, 0.00124383,\n",
              "        0.00121498, 0.00154018, 0.00173521, 0.00223994, 0.00121593,\n",
              "        0.00123286, 0.00121689, 0.00144005, 0.00138807, 0.00126076,\n",
              "        0.00125384, 0.00145698, 0.00124192, 0.00266504, 0.00127792,\n",
              "        0.00242114, 0.00194502, 0.002882  , 0.00125194, 0.0012579 ,\n",
              "        0.00219488, 0.00131798, 0.00121713, 0.01335788, 0.0012238 ,\n",
              "        0.00131989, 0.00231123, 0.00126719, 0.00131202, 0.00140095,\n",
              "        0.00121498, 0.00159168, 0.0020752 , 0.00143504, 0.00139689,\n",
              "        0.00154018, 0.00120711, 0.00123596, 0.00121212, 0.00120401,\n",
              "        0.00121999, 0.00121117, 0.00181985, 0.00122023, 0.00135708,\n",
              "        0.00129795, 0.00156403, 0.00126505, 0.00172687, 0.00126123,\n",
              "        0.00145102, 0.00252318, 0.00121117, 0.00159502, 0.00190806,\n",
              "        0.001302  , 0.0012219 , 0.00208211, 0.00121903, 0.00123501,\n",
              "        0.0020988 , 0.0014081 , 0.00180578, 0.00134492, 0.00121999,\n",
              "        0.00121903, 0.00125885, 0.00189519, 0.00128007, 0.00171328,\n",
              "        0.00121999, 0.00125098, 0.00143504, 0.00149894, 0.00274897,\n",
              "        0.00122809, 0.00141406, 0.00121593, 0.00122714, 0.00131702,\n",
              "        0.00122881, 0.00121593, 0.00181699, 0.00271106, 0.00133419,\n",
              "        0.00174284, 0.00120711, 0.00131989, 0.00225401, 0.00125289,\n",
              "        0.0012219 , 0.00229025, 0.00124383, 0.00125575, 0.00125909,\n",
              "        0.00129795, 0.00133705, 0.00173616, 0.00179601, 0.00268173,\n",
              "        0.00121307, 0.00131679, 0.00135779, 0.00121784, 0.00169277,\n",
              "        0.00150394, 0.00128007, 0.00121498, 0.00255322, 0.00121188,\n",
              "        0.00122094, 0.0012238 , 0.00125813, 0.00194597, 0.00121307,\n",
              "        0.00218415, 0.00122809, 0.00256681, 0.00138593, 0.00122499,\n",
              "        0.001652  , 0.00121379, 0.00122428, 0.00120783, 0.00124574,\n",
              "        0.00171185, 0.00125694, 0.00133395, 0.00160098, 0.00213814,\n",
              "        0.00179195, 0.00130415, 0.00134516, 0.00120974, 0.00209904,\n",
              "        0.00187922, 0.001755  , 0.00121093, 0.00184512, 0.00121069,\n",
              "        0.00138307, 0.00121188, 0.00120592, 0.00180411, 0.00134706,\n",
              "        0.0012219 , 0.00121617, 0.00128627, 0.00124192, 0.00134516,\n",
              "        0.002527  , 0.00148797, 0.00194097, 0.00132012, 0.00122809,\n",
              "        0.00154638, 0.00162101, 0.00156808, 0.00123215, 0.00120592,\n",
              "        0.00170922, 0.00128293, 0.00120807, 0.00192285, 0.00125599,\n",
              "        0.00126505, 0.00132608, 0.00177717, 0.0013361 , 0.00209403,\n",
              "        0.00137687, 0.00123119, 0.00122809, 0.00139499, 0.00139189,\n",
              "        0.00121975, 0.00195312, 0.00240397, 0.00285292, 0.00147319,\n",
              "        0.00172091, 0.00163507, 0.00192904, 0.00177407, 0.00121784,\n",
              "        0.00182891, 0.00123   , 0.00141907, 0.00144315, 0.00137186,\n",
              "        0.00121903, 0.00122118, 0.00135899, 0.00209928, 0.00190163,\n",
              "        0.00163603, 0.00135708, 0.00134039, 0.00121808, 0.00200486,\n",
              "        0.00229907, 0.00129199, 0.00122213, 0.00122118, 0.00122309,\n",
              "        0.00170612, 0.00142908, 0.00120711, 0.00150514, 0.00195503,\n",
              "        0.00120497, 0.00289321, 0.00126076, 0.00136089, 0.00120902,\n",
              "        0.00132704, 0.00132608, 0.00196218, 0.00244594, 0.00198793,\n",
              "        0.0015161 , 0.00125694, 0.00181675, 0.00150609, 0.0014658 ,\n",
              "        0.00120974, 0.00121331, 0.00176191, 0.00189781, 0.00123501,\n",
              "        0.00121593, 0.00133014, 0.00186491, 0.00126672, 0.00195718,\n",
              "        0.00121903, 0.00136375, 0.00134087, 0.00215006, 0.00124121,\n",
              "        0.00149179, 0.00140595, 0.00121307, 0.00122285, 0.00125098,\n",
              "        0.00122786, 0.00134587, 0.00121379, 0.00122619, 0.00120401,\n",
              "        0.00206089, 0.00137115, 0.00149107, 0.00127792, 0.00121188,\n",
              "        0.00122595, 0.00137615, 0.00137019, 0.00121617, 0.00124025,\n",
              "        0.00265503, 0.00123596, 0.00121069, 0.003613  , 0.0012207 ,\n",
              "        0.00169802, 0.00181293, 0.00121307, 0.00120807, 0.00152707,\n",
              "        0.00121403, 0.00134826, 0.00199771, 0.00121021, 0.00122499,\n",
              "        0.00216293, 0.00194573, 0.00123286, 0.00125289, 0.00134993,\n",
              "        0.00122213, 0.00123382, 0.00124121, 0.001261  , 0.00136423,\n",
              "        0.00130606, 0.00129271, 0.00133181, 0.00238013, 0.00216818,\n",
              "        0.00213814, 0.00139308, 0.00290799, 0.00251603, 0.00233698,\n",
              "        0.00138807, 0.00697589, 0.00175786, 0.00180984, 0.00130987,\n",
              "        0.00142431, 0.00123811, 0.00120807, 0.00121999, 0.00178218,\n",
              "        0.001369  , 0.00135207, 0.00166607, 0.00146198, 0.00188589,\n",
              "        0.00190687, 0.00129819, 0.00121498, 0.00250006, 0.00202703,\n",
              "        0.00188684, 0.00140309, 0.00122714, 0.00121188, 0.00132895,\n",
              "        0.00121784, 0.00120401, 0.00183797, 0.00121284, 0.00127578,\n",
              "        0.00181913, 0.00191402, 0.00160527, 0.00207496, 0.00158215,\n",
              "        0.00121307, 0.001333  , 0.00120401, 0.00121212, 0.00122714,\n",
              "        0.00220084, 0.00122285, 0.00276375, 0.00128889, 0.00200605,\n",
              "        0.00126791, 0.00193405, 0.00120616, 0.0012958 , 0.0013299 ,\n",
              "        0.00123525, 0.00194311, 0.00181913, 0.00121498, 0.00344205,\n",
              "        0.00215411, 0.00214505, 0.00178313, 0.00275993, 0.00134993,\n",
              "        0.00161386, 0.00168681, 0.00122905, 0.0012219 , 0.002213  ,\n",
              "        0.00230122, 0.00143504, 0.00134087, 0.00140977, 0.00192809,\n",
              "        0.0012188 , 0.00147295, 0.00265217, 0.00136495, 0.00199389,\n",
              "        0.00173092, 0.0012548 , 0.00122905, 0.0012579 , 0.00120807,\n",
              "        0.0012219 , 0.00146294, 0.0014751 , 0.00127792, 0.00123191,\n",
              "        0.00121307, 0.00120401, 0.00135517, 0.00243711, 0.00136018,\n",
              "        0.0016582 , 0.00175691, 0.00145793, 0.00121784, 0.00171399,\n",
              "        0.00254416, 0.00130796, 0.00122619, 0.00145793, 0.00121379,\n",
              "        0.00123119, 0.00123334, 0.00122285, 0.00121808, 0.00121689,\n",
              "        0.00157213, 0.00183511, 0.00120902, 0.00155306, 0.00213194,\n",
              "        0.00208116, 0.00128722, 0.00121975, 0.00122595, 0.00181198,\n",
              "        0.00121593, 0.00131416, 0.00136399, 0.00153589, 0.00163794,\n",
              "        0.001302  , 0.00120711, 0.00133705, 0.00133896, 0.00135207,\n",
              "        0.00132585, 0.00121689, 0.00134921, 0.00165582, 0.00128317,\n",
              "        0.00122666, 0.00121713, 0.00121403, 0.00120878, 0.00125885,\n",
              "        0.00134015, 0.00121403, 0.00122905, 0.00124598, 0.00126219,\n",
              "        0.00177622, 0.00175595, 0.00119781, 0.00134921, 0.00132084,\n",
              "        0.00124478, 0.00139904, 0.00132775, 0.00122881, 0.00123429,\n",
              "        0.0012002 , 0.0012219 , 0.00122595, 0.00120807, 0.00120497,\n",
              "        0.00120807, 0.00120807, 0.00120807, 0.00123191, 0.00123501,\n",
              "        0.0013299 , 0.00139761, 0.00128269, 0.00129676, 0.00120807,\n",
              "        0.00156999, 0.00121427, 0.00158596, 0.00122094, 0.0012002 ,\n",
              "        0.00120091, 0.0012269 , 0.00120497, 0.00120926, 0.00142002,\n",
              "        0.00120974, 0.00121403, 0.00121093, 0.00122523, 0.00122786,\n",
              "        0.00131679, 0.00178313, 0.00121498, 0.00131798, 0.00142002,\n",
              "        0.00179601, 0.00121093, 0.0012989 , 0.00127673, 0.00180507,\n",
              "        0.00168109, 0.0012188 , 0.00122333, 0.00132203, 0.00137091,\n",
              "        0.00160885, 0.00182605, 0.00123787, 0.0014441 , 0.0017941 ,\n",
              "        0.00162292, 0.00193024, 0.00157285, 0.00142813, 0.00120497,\n",
              "        0.00128508, 0.00120664, 0.00145411, 0.00124407, 0.00138307,\n",
              "        0.00120425, 0.00124192, 0.00148511, 0.00121689, 0.00123   ,\n",
              "        0.00142407, 0.00151706, 0.00132608, 0.00124693, 0.00123715,\n",
              "        0.00177407, 0.0037322 , 0.00170398, 0.00121808, 0.00163579,\n",
              "        0.00130987, 0.00209618, 0.0016861 , 0.00173211, 0.00143909,\n",
              "        0.00120878, 0.00122929, 0.00205994, 0.00121593, 0.00216413,\n",
              "        0.00120282, 0.00120401, 0.00136614, 0.00222611, 0.00121188,\n",
              "        0.00145888, 0.00120974, 0.0013628 , 0.00123215, 0.00121117,\n",
              "        0.00122118, 0.00129795, 0.00162005, 0.00120902, 0.00123906,\n",
              "        0.00195408, 0.0012331 , 0.00133586, 0.0013752 , 0.00186419,\n",
              "        0.00120997, 0.00192118, 0.00123978, 0.00121999, 0.0018611 ,\n",
              "        0.00123   , 0.00213575, 0.00126505, 0.00121498, 0.00135016,\n",
              "        0.00129199, 0.00122476, 0.00139499, 0.00153399, 0.00120783,\n",
              "        0.00138593, 0.00133896, 0.0022788 , 0.00199509, 0.001755  ,\n",
              "        0.00125408, 0.00131893, 0.00125623, 0.00153399, 0.00122309,\n",
              "        0.00125694, 0.00121379, 0.00120521, 0.00204992, 0.0014689 ,\n",
              "        0.00121307, 0.00135303, 0.00119781, 0.00221372, 0.001791  ,\n",
              "        0.00124812, 0.00217915, 0.00134993]),\n",
              " 'test_accuracy': array([1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
              "        0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
              "        1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
              "        1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
              "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
              "        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
              "        1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
              "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
              "        0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
              "        0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
              "        0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
              "        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
              "        0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
              "        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
              "        1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
              "        1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
              "        0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
              "        0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
              "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
              "        1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
              "        1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
              "        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
              "        0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
              "        0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
              "        0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
              "        0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
              "        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
              "        1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
              "        0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
              "        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
              "        0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
              "        1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 0., 1.])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VXdvPbqgh6F",
        "colab_type": "code",
        "colab": {},
        "outputId": "1423c3fa-5580-4555-f6f6-8a039934f257"
      },
      "source": [
        "cv_results['test_accuracy'].mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7200520833333334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh7CwmWygh6b",
        "colab_type": "text"
      },
      "source": [
        "We have not included precision and recall in the metrics here. Can you think why?  \n",
        "**<mark>Hint:</mark> Imagine the confusion matrix when the testing has only one sample**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1GyHfC0gh6j",
        "colab_type": "text"
      },
      "source": [
        "# 3. Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEHcCWcegh6l",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 Grid Search CV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQwxdMUkgh6m",
        "colab_type": "text"
      },
      "source": [
        "### 3.1.1 Crime Rate- Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qVEuJimgh6n",
        "colab_type": "text"
      },
      "source": [
        "**Predictor Variable: Crime Rate (Regression Based)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Wbh15ZTgh6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NamrnJjOgh6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "crime = pd.read_csv(\"Standard Metropolitan Areas Data - train_data.csv\")\n",
        "train, test = train_test_split(crime)\n",
        "x_train = train.iloc[:,:-1]\n",
        "y_train = np.array(train.crime_rate.values).reshape(len(x_train),1)\n",
        "x_test = test.iloc[:,:-1]\n",
        "y_test = np.array(test.crime_rate.values).reshape(len(x_test),1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DZvhZ0fgh66",
        "colab_type": "text"
      },
      "source": [
        "Performance without grid search: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TjevVRegh67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "y_pred = lr.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2rzRQoBgh7D",
        "colab_type": "code",
        "colab": {},
        "outputId": "9b5ce3a7-0bcb-4e0f-dd25-e975136f381f"
      },
      "source": [
        "mean_squared_error(y_test, y_pred, squared=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.507370658524252"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no8Vw1Qmgh7N",
        "colab_type": "text"
      },
      "source": [
        "Performance with Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UPswE7Ogh7O",
        "colab_type": "text"
      },
      "source": [
        "**Step 1:** Define a parameter Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6084pRAvgh7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False], 'n_jobs':[-1,1,10,15]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPQl8cdHgh7Y",
        "colab_type": "text"
      },
      "source": [
        "**Step 2:** Fit the model to find the best hyperparameters on training data, and select the scorer you want to select to optimise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSWCh9rDgh7Z",
        "colab_type": "code",
        "colab": {},
        "outputId": "0ec0e627-0118-4657-f38b-32f014ed6d18"
      },
      "source": [
        "grid = GridSearchCV(lr,parameters, cv=3)\n",
        "grid.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=3, error_score=nan,\n",
              "             estimator=LinearRegression(copy_X=True, fit_intercept=True,\n",
              "                                        n_jobs=None, normalize=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'copy_X': [True, False],\n",
              "                         'fit_intercept': [True, False],\n",
              "                         'n_jobs': [-1, 1, 10, 15],\n",
              "                         'normalize': [True, False]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTdZBb40gh7d",
        "colab_type": "text"
      },
      "source": [
        "**Step 3:** Print the best obtained parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIonXbIogh7f",
        "colab_type": "code",
        "colab": {},
        "outputId": "823f73d1-2133-402a-be40-478da722f90b"
      },
      "source": [
        "grid.best_estimator_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Urv2jtngh7n",
        "colab_type": "code",
        "colab": {},
        "outputId": "a1191b6d-b59c-4cea-9991-691197369498"
      },
      "source": [
        "grid_lr = LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=False)\n",
        "grid_lr.fit(x_train, y_train)\n",
        "y_pred= grid_lr.predict(x_test)\n",
        "mean_squared_error(y_test, y_pred, squared=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.507370658524252"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwhEjnmGgh7v",
        "colab_type": "text"
      },
      "source": [
        "**Performance does not vary that much!**\n",
        "\n",
        "The number of hyperparameters for Linear Regression is very less. Hence all of them give similar performance (in this specific dataset)\n",
        "\n",
        "Let us try another parameter for which the performance varies a lot!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gp-Po0wgh7w",
        "colab_type": "text"
      },
      "source": [
        "### 3.1.2 Artificial Neural Network\n",
        "In Linear Regression, there are not many parameters to optimise, hence performance may not vary that much. In many other classifiers, there are a number of hyper parameters to tune, so let us see an example of how performance is improved using Grid Search. We take an example of **Artificial Neural Networks.**\n",
        "\n",
        "You need not understand the working behind ANN, so it is okay if you do not understand the parameter grid in detail. Let's just see how the performance improves by applying Grid Search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2W1Ei3Ugh7y",
        "colab_type": "text"
      },
      "source": [
        "**Step 1:** Define a parameter Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX_8tDiSgh7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameter_space = {\n",
        "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'alpha': [0.0001, 0.05],\n",
        "    'learning_rate': ['constant','adaptive'],\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kcl_8hc6gh79",
        "colab_type": "text"
      },
      "source": [
        "**Step 2:** Fit the model to find the best hyperparameters on training data, and select the scorer you want to select to optimise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc9VN84Ygh7-",
        "colab_type": "text"
      },
      "source": [
        "<blockquote> <i>  This code takes a long time to run, you can either skip running this part and directly just see the printed results, or wait for 10-15 mins for this to run </blockquote>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBeiT6D1gh7_",
        "colab_type": "code",
        "colab": {},
        "outputId": "e9d7e005-c747-4cdf-d16c-001dc2cb1eae"
      },
      "source": [
        "mlp_random = GridSearchCV(mlp, parameter_space, scoring = 'accuracy')\n",
        "mlp_random.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=None, error_score=nan,\n",
              "             estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
              "                                     batch_size='auto', beta_1=0.9,\n",
              "                                     beta_2=0.999, early_stopping=False,\n",
              "                                     epsilon=1e-08, hidden_layer_sizes=(100,),\n",
              "                                     learning_rate='constant',\n",
              "                                     learning_rate_init=0.001, max_fun=15000,\n",
              "                                     max_iter=1000, momentum=0.9,\n",
              "                                     n_iter_no_change=10,\n",
              "                                     nesterovs_momentum=True, power_t=0.5,\n",
              "                                     random_s...\n",
              "                                     validation_fraction=0.1, verbose=False,\n",
              "                                     warm_start=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'activation': ['tanh', 'relu'],\n",
              "                         'alpha': [0.0001, 0.05],\n",
              "                         'hidden_layer_sizes': [(50, 50, 50), (50, 100, 50),\n",
              "                                                (100,)],\n",
              "                         'learning_rate': ['constant', 'adaptive'],\n",
              "                         'solver': ['sgd', 'adam']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='accuracy', verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8f-duE9gh8J",
        "colab_type": "text"
      },
      "source": [
        "**Step 3:** Print the best obtained parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4bj3qClgh8K",
        "colab_type": "code",
        "colab": {},
        "outputId": "cdc1b999-45a2-4b58-8cf6-028bfe4c7b73"
      },
      "source": [
        "mlp_random.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'tanh',\n",
              " 'alpha': 0.0001,\n",
              " 'hidden_layer_sizes': (100,),\n",
              " 'learning_rate': 'constant',\n",
              " 'solver': 'adam'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmHmS7Nfgh8S",
        "colab_type": "text"
      },
      "source": [
        "**Step 4:** Train your model on these parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2repqpLogh8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp_grid = MLPClassifier(solver='adam', learning_rate='constant', hidden_layer_sizes=(100,), alpha=0.0001, \n",
        "                         activation='tanh',max_iter=2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCXtKB2Ugh8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp_grid.fit(x_train, y_train)\n",
        "y_pred = mlp_grid.predict(x_test)\n",
        "acc_tuned = accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZTgmXQogh8e",
        "colab_type": "text"
      },
      "source": [
        "**Comparing with Accuracy from model without hyperparameter tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "H-kojEW-gh8f",
        "colab_type": "code",
        "colab": {},
        "outputId": "332e68b1-4834-4c47-f37b-60d79263bcd8"
      },
      "source": [
        "print(\"Accuracy of Tuned model: \",np.round(acc_tuned,3))\n",
        "print(\"Accuracy of non-Tuned model: \",np.round(acc,3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Tuned model:  0.714\n",
            "Accuracy of non-Tuned model:  0.656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsGwxzNOgh8h",
        "colab_type": "text"
      },
      "source": [
        "Approximately 5% difference in accuracy!  \n",
        "By including an even more exhaustive grid search, we can improve the performance even further"
      ]
    }
  ]
}